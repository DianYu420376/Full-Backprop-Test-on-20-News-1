{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training a fully supervised one layer NMF on 20 news group dataset\n",
    "'''\n",
    "# define some global variables\n",
    "save_PATH = 'saved_data/'\n",
    "save_filename = 'supervised_one_layer_pinv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "# import package\n",
    "%load_ext memory_profiler\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import Ipynb_importer\n",
    "from deep_nmf import Deep_NMF, Energy_Loss_Func, Fro_Norm\n",
    "from writer import Writer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from auxillary_functions import *\n",
    "from pinv import PinvF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset for twenty news\n",
    "from twenty_news_group_data_loading import data, Y, target,L20, L50, L90, sparsedata_cr_entr, sparsedata_L2#, get_whole_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network \n",
    "m = data.shape[1]\n",
    "k = 20\n",
    "c = 20\n",
    "lambd = 1e-4\n",
    "net = Deep_NMF([m, k])\n",
    "loss_func = Energy_Loss_Func(lambd = lambd, classification_type = 'L2')\n",
    "data_input = data*1000\n",
    "dataset = sparsedata_L2(data_input, 1000*Y)\n",
    "criterion = Fro_Norm()\n",
    "pinv = PinvF.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try initializing the network with the unsupervised version\n",
    "#A = np.load(save_PATH + '20_news_group_A.npy')\n",
    "#net.lsqnonneglst[0].A.data = torch.from_numpy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 1 tensor(11.4501, dtype=torch.float64)\n",
      "25.679558277130127\n",
      "current at batch: 2 tensor(11.3143, dtype=torch.float64)\n",
      "13.477495193481445\n",
      "current at batch: 3 tensor(11.3293, dtype=torch.float64)\n",
      "14.196966409683228\n",
      "current at batch: 4 tensor(11.3330, dtype=torch.float64)\n",
      "12.990583419799805\n",
      "current at batch: 5 tensor(11.2551, dtype=torch.float64)\n",
      "12.769751071929932\n",
      "current at batch: 6 tensor(11.2910, dtype=torch.float64)\n",
      "12.480274438858032\n",
      "current at batch: 7 tensor(11.1565, dtype=torch.float64)\n",
      "12.017467737197876\n",
      "current at batch: 8 tensor(11.1865, dtype=torch.float64)\n",
      "11.961984634399414\n",
      "current at batch: 9 tensor(11.2308, dtype=torch.float64)\n",
      "10.82209300994873\n",
      "current at batch: 10 tensor(11.0916, dtype=torch.float64)\n",
      "10.217730522155762\n",
      "current at batch: 11 tensor(11.0197, dtype=torch.float64)\n",
      "10.347598791122437\n",
      "current at batch: 12 tensor(11.0759, dtype=torch.float64)\n",
      "10.511369705200195\n",
      "current at batch: 13 tensor(10.8860, dtype=torch.float64)\n",
      "9.830856323242188\n",
      "current at batch: 14 tensor(10.5608, dtype=torch.float64)\n",
      "9.619498252868652\n",
      "current at batch: 15 tensor(10.4777, dtype=torch.float64)\n",
      "9.320650100708008\n",
      "current at batch: 16 tensor(10.1527, dtype=torch.float64)\n",
      "9.298012018203735\n",
      "current at batch: 17 tensor(9.9973, dtype=torch.float64)\n",
      "8.992569208145142\n",
      "current at batch: 18 tensor(9.8263, dtype=torch.float64)\n",
      "9.005903482437134\n",
      "current at batch: 19 tensor(9.9787, dtype=torch.float64)\n",
      "9.193741798400879\n",
      "current at batch: 20 tensor(9.6254, dtype=torch.float64)\n",
      "8.810017824172974\n",
      "current at batch: 21 tensor(9.5803, dtype=torch.float64)\n",
      "8.982099533081055\n",
      "current at batch: 22 tensor(9.2775, dtype=torch.float64)\n",
      "9.36882209777832\n",
      "current at batch: 23 tensor(9.2807, dtype=torch.float64)\n",
      "9.364664554595947\n",
      "current at batch: 24 tensor(9.0092, dtype=torch.float64)\n",
      "8.945514440536499\n",
      "current at batch: 25 tensor(9.0853, dtype=torch.float64)\n",
      "9.647352695465088\n",
      "current at batch: 26 tensor(9.0935, dtype=torch.float64)\n",
      "9.551521301269531\n",
      "current at batch: 27 tensor(9.2159, dtype=torch.float64)\n",
      "9.492636680603027\n",
      "current at batch: 28 tensor(8.8365, dtype=torch.float64)\n",
      "10.307378053665161\n",
      "current at batch: 29 tensor(9.1652, dtype=torch.float64)\n",
      "10.431366920471191\n",
      "current at batch: 30 tensor(8.8519, dtype=torch.float64)\n",
      "9.989416599273682\n",
      "current at batch: 31 tensor(9.2088, dtype=torch.float64)\n",
      "10.234501600265503\n",
      "current at batch: 32 tensor(9.0206, dtype=torch.float64)\n",
      "10.713242530822754\n",
      "current at batch: 33 tensor(8.6048, dtype=torch.float64)\n",
      "11.697263717651367\n",
      "current at batch: 34 tensor(8.9322, dtype=torch.float64)\n",
      "10.467955112457275\n",
      "current at batch: 35 tensor(8.7482, dtype=torch.float64)\n",
      "10.968217134475708\n",
      "current at batch: 36 tensor(8.9736, dtype=torch.float64)\n",
      "12.354097127914429\n",
      "current at batch: 37 tensor(8.8117, dtype=torch.float64)\n",
      "11.755982875823975\n",
      "current at batch: 38 tensor(8.8152, dtype=torch.float64)\n",
      "12.324956178665161\n",
      "current at batch: 39 tensor(8.7169, dtype=torch.float64)\n",
      "12.842041254043579\n",
      "current at batch: 40 tensor(8.9360, dtype=torch.float64)\n",
      "11.704126119613647\n",
      "current at batch: 41 tensor(8.5908, dtype=torch.float64)\n",
      "11.136651754379272\n",
      "current at batch: 42 tensor(8.6787, dtype=torch.float64)\n",
      "12.099684238433838\n",
      "current at batch: 43 tensor(8.7828, dtype=torch.float64)\n",
      "12.164697408676147\n",
      "current at batch: 44 tensor(8.5295, dtype=torch.float64)\n",
      "12.544533967971802\n",
      "current at batch: 45 tensor(8.7806, dtype=torch.float64)\n",
      "12.962159633636475\n",
      "current at batch: 46 tensor(8.7633, dtype=torch.float64)\n",
      "11.806774139404297\n",
      "current at batch: 47 tensor(8.8209, dtype=torch.float64)\n",
      "12.300493001937866\n",
      "current at batch: 48 tensor(8.5649, dtype=torch.float64)\n",
      "11.312073945999146\n",
      "current at batch: 49 tensor(8.5327, dtype=torch.float64)\n",
      "13.202331066131592\n",
      "current at batch: 50 tensor(8.5236, dtype=torch.float64)\n",
      "12.478914260864258\n",
      "current at batch: 51 tensor(8.6691, dtype=torch.float64)\n",
      "11.927868843078613\n",
      "current at batch: 52 tensor(9.0774, dtype=torch.float64)\n",
      "12.000518321990967\n",
      "current at batch: 53 tensor(8.4458, dtype=torch.float64)\n",
      "11.745041847229004\n",
      "current at batch: 54 tensor(8.5115, dtype=torch.float64)\n",
      "12.32446026802063\n",
      "current at batch: 55 tensor(8.7088, dtype=torch.float64)\n",
      "10.921626329421997\n",
      "current at batch: 56 tensor(8.5437, dtype=torch.float64)\n",
      "11.738317966461182\n",
      "current at batch: 57 tensor(8.4974, dtype=torch.float64)\n",
      "11.958321809768677\n",
      "current at batch: 58 tensor(8.4396, dtype=torch.float64)\n",
      "12.624939918518066\n",
      "current at batch: 59 tensor(8.4274, dtype=torch.float64)\n",
      "12.31706428527832\n",
      "current at batch: 60 tensor(8.3619, dtype=torch.float64)\n",
      "11.752156972885132\n",
      "current at batch: 61 tensor(8.2822, dtype=torch.float64)\n",
      "13.131017923355103\n",
      "current at batch: 62 tensor(8.3580, dtype=torch.float64)\n",
      "13.40755033493042\n",
      "current at batch: 63 tensor(8.6433, dtype=torch.float64)\n",
      "11.392174243927002\n",
      "current at batch: 64 tensor(8.3146, dtype=torch.float64)\n",
      "12.943726539611816\n",
      "current at batch: 65 tensor(8.4879, dtype=torch.float64)\n",
      "12.077805280685425\n",
      "current at batch: 66 tensor(8.4003, dtype=torch.float64)\n",
      "12.45933723449707\n",
      "current at batch: 67 tensor(8.6817, dtype=torch.float64)\n",
      "13.109524488449097\n",
      "current at batch: 68 tensor(8.7147, dtype=torch.float64)\n",
      "12.949889659881592\n",
      "current at batch: 69 tensor(8.0013, dtype=torch.float64)\n",
      "12.688152074813843\n",
      "current at batch: 70 tensor(8.2131, dtype=torch.float64)\n",
      "12.021788120269775\n",
      "current at batch: 71 tensor(8.7586, dtype=torch.float64)\n",
      "12.841926574707031\n",
      "current at batch: 72 tensor(8.2453, dtype=torch.float64)\n",
      "13.911842107772827\n",
      "current at batch: 73 tensor(8.4892, dtype=torch.float64)\n",
      "13.79808235168457\n",
      "current at batch: 74 tensor(8.2104, dtype=torch.float64)\n",
      "14.286470413208008\n",
      "current at batch: 75 tensor(8.3007, dtype=torch.float64)\n",
      "14.28801155090332\n",
      "current at batch: 76 tensor(8.3652, dtype=torch.float64)\n",
      "12.97438907623291\n",
      "current at batch: 77 tensor(8.4713, dtype=torch.float64)\n",
      "13.520878314971924\n",
      "current at batch: 78 tensor(8.2761, dtype=torch.float64)\n",
      "14.5684335231781\n",
      "current at batch: 79 tensor(8.1806, dtype=torch.float64)\n",
      "13.718754291534424\n",
      "current at batch: 80 tensor(8.1092, dtype=torch.float64)\n",
      "13.533353328704834\n",
      "current at batch: 81 tensor(8.3468, dtype=torch.float64)\n",
      "13.799296617507935\n",
      "current at batch: 82 tensor(8.0973, dtype=torch.float64)\n",
      "14.280409574508667\n",
      "current at batch: 83 tensor(8.3973, dtype=torch.float64)\n",
      "14.798549175262451\n",
      "current at batch: 84 tensor(8.0952, dtype=torch.float64)\n",
      "13.844823122024536\n",
      "current at batch: 85 tensor(7.9964, dtype=torch.float64)\n",
      "13.177988767623901\n",
      "current at batch: 86 tensor(8.1376, dtype=torch.float64)\n",
      "12.827168703079224\n",
      "current at batch: 87 tensor(7.8617, dtype=torch.float64)\n",
      "13.797170162200928\n",
      "current at batch: 88 tensor(7.9266, dtype=torch.float64)\n",
      "13.18904733657837\n",
      "current at batch: 89 tensor(8.3079, dtype=torch.float64)\n",
      "13.935050249099731\n",
      "current at batch: 90 tensor(7.9688, dtype=torch.float64)\n",
      "13.579567909240723\n",
      "current at batch: 91 tensor(7.7889, dtype=torch.float64)\n",
      "13.975033521652222\n",
      "current at batch: 92 tensor(8.1466, dtype=torch.float64)\n",
      "14.297297477722168\n",
      "current at batch: 93 tensor(7.9282, dtype=torch.float64)\n",
      "13.998974323272705\n",
      "current at batch: 94 tensor(7.9911, dtype=torch.float64)\n",
      "13.705502986907959\n",
      "current at batch: 95 tensor(8.2229, dtype=torch.float64)\n",
      "14.094810485839844\n",
      "current at batch: 96 tensor(7.8362, dtype=torch.float64)\n",
      "13.246137857437134\n",
      "current at batch: 97 tensor(7.8629, dtype=torch.float64)\n",
      "14.339950323104858\n",
      "current at batch: 98 tensor(8.0148, dtype=torch.float64)\n",
      "13.599000215530396\n",
      "current at batch: 99 tensor(7.9653, dtype=torch.float64)\n",
      "14.012181758880615\n",
      "current at batch: 100 tensor(8.2513, dtype=torch.float64)\n",
      "14.342913150787354\n",
      "current at batch: 101 tensor(7.8164, dtype=torch.float64)\n",
      "13.896909236907959\n",
      "current at batch: 102 tensor(8.0451, dtype=torch.float64)\n",
      "13.31378173828125\n",
      "current at batch: 103 tensor(8.0067, dtype=torch.float64)\n",
      "13.204102277755737\n",
      "current at batch: 104 tensor(8.1087, dtype=torch.float64)\n",
      "13.859561443328857\n",
      "current at batch: 105 tensor(7.8079, dtype=torch.float64)\n",
      "14.030991077423096\n",
      "current at batch: 106 tensor(7.8759, dtype=torch.float64)\n",
      "14.172196388244629\n",
      "current at batch: 107 tensor(8.3326, dtype=torch.float64)\n",
      "14.12491488456726\n",
      "current at batch: 108 tensor(7.8983, dtype=torch.float64)\n",
      "14.599194288253784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 109 tensor(7.7673, dtype=torch.float64)\n",
      "13.927613973617554\n",
      "current at batch: 110 tensor(8.3270, dtype=torch.float64)\n",
      "15.43793535232544\n",
      "current at batch: 111 tensor(8.0988, dtype=torch.float64)\n",
      "14.280504703521729\n",
      "current at batch: 112 tensor(7.7581, dtype=torch.float64)\n",
      "12.687820434570312\n",
      "current at batch: 113 tensor(8.1627, dtype=torch.float64)\n",
      "13.215121030807495\n",
      "current at batch: 114 tensor(8.1979, dtype=torch.float64)\n",
      "13.277462482452393\n",
      "current at batch: 115 tensor(7.9704, dtype=torch.float64)\n",
      "13.625702857971191\n",
      "current at batch: 116 tensor(7.6458, dtype=torch.float64)\n",
      "13.241592168807983\n",
      "current at batch: 117 tensor(8.0437, dtype=torch.float64)\n",
      "14.375263214111328\n",
      "current at batch: 118 tensor(7.8797, dtype=torch.float64)\n",
      "13.49994969367981\n",
      "current at batch: 119 tensor(7.6984, dtype=torch.float64)\n",
      "13.750902891159058\n",
      "current at batch: 120 tensor(7.8302, dtype=torch.float64)\n",
      "13.266018390655518\n",
      "current at batch: 121 tensor(7.9424, dtype=torch.float64)\n",
      "13.744791030883789\n",
      "current at batch: 122 tensor(7.7273, dtype=torch.float64)\n",
      "13.48935866355896\n",
      "current at batch: 123 tensor(8.0226, dtype=torch.float64)\n",
      "14.298138618469238\n",
      "current at batch: 124 tensor(7.5673, dtype=torch.float64)\n",
      "13.233832597732544\n",
      "current at batch: 125 tensor(7.2374, dtype=torch.float64)\n",
      "14.843377828598022\n",
      "current at batch: 126 tensor(7.6359, dtype=torch.float64)\n",
      "13.140310049057007\n",
      "current at batch: 127 tensor(8.0063, dtype=torch.float64)\n",
      "14.547061681747437\n",
      "current at batch: 128 tensor(7.8315, dtype=torch.float64)\n",
      "14.484879732131958\n",
      "current at batch: 129 tensor(7.6492, dtype=torch.float64)\n",
      "14.53136134147644\n",
      "current at batch: 130 tensor(7.7531, dtype=torch.float64)\n",
      "13.848454236984253\n",
      "current at batch: 131 tensor(7.3837, dtype=torch.float64)\n",
      "14.549684047698975\n",
      "current at batch: 132 tensor(7.7527, dtype=torch.float64)\n",
      "14.533103942871094\n",
      "current at batch: 133 tensor(7.6285, dtype=torch.float64)\n",
      "14.316926956176758\n",
      "current at batch: 134 tensor(7.6549, dtype=torch.float64)\n",
      "13.84764552116394\n",
      "current at batch: 135 tensor(7.5197, dtype=torch.float64)\n",
      "13.938109636306763\n",
      "current at batch: 136 tensor(7.7827, dtype=torch.float64)\n",
      "14.794662475585938\n",
      "current at batch: 137 tensor(7.6544, dtype=torch.float64)\n",
      "14.141911268234253\n",
      "current at batch: 138 tensor(7.8595, dtype=torch.float64)\n",
      "14.150968551635742\n",
      "current at batch: 139 tensor(7.9400, dtype=torch.float64)\n",
      "14.624062299728394\n",
      "current at batch: 140 tensor(7.7294, dtype=torch.float64)\n",
      "14.143580913543701\n",
      "current at batch: 141 tensor(7.6636, dtype=torch.float64)\n",
      "14.760571718215942\n",
      "current at batch: 142 tensor(7.6497, dtype=torch.float64)\n",
      "13.253109216690063\n",
      "current at batch: 143 tensor(7.7459, dtype=torch.float64)\n",
      "13.975865364074707\n",
      "current at batch: 144 tensor(7.7483, dtype=torch.float64)\n",
      "14.693892002105713\n",
      "current at batch: 145 tensor(7.7839, dtype=torch.float64)\n",
      "14.270063638687134\n",
      "current at batch: 146 tensor(7.1574, dtype=torch.float64)\n",
      "14.76745343208313\n",
      "current at batch: 147 tensor(7.4778, dtype=torch.float64)\n",
      "14.053016424179077\n",
      "current at batch: 148 tensor(7.5684, dtype=torch.float64)\n",
      "13.660352230072021\n",
      "current at batch: 149 tensor(7.1244, dtype=torch.float64)\n",
      "14.881326675415039\n",
      "current at batch: 150 tensor(7.2911, dtype=torch.float64)\n",
      "14.130195617675781\n",
      "current at batch: 151 tensor(7.8511, dtype=torch.float64)\n",
      "14.615358352661133\n",
      "current at batch: 152 tensor(8.1086, dtype=torch.float64)\n",
      "15.270406723022461\n",
      "current at batch: 153 tensor(7.2803, dtype=torch.float64)\n",
      "14.899662017822266\n",
      "current at batch: 154 tensor(7.7610, dtype=torch.float64)\n",
      "14.791199684143066\n",
      "current at batch: 155 tensor(7.8872, dtype=torch.float64)\n",
      "15.507217168807983\n",
      "current at batch: 156 tensor(7.7587, dtype=torch.float64)\n",
      "14.330447912216187\n",
      "current at batch: 157 tensor(7.3326, dtype=torch.float64)\n",
      "14.524009466171265\n",
      "current at batch: 158 tensor(7.1852, dtype=torch.float64)\n",
      "14.89041256904602\n",
      "current at batch: 159 tensor(7.7798, dtype=torch.float64)\n",
      "15.20887804031372\n",
      "current at batch: 160 tensor(7.5581, dtype=torch.float64)\n",
      "13.91136360168457\n",
      "current at batch: 161 tensor(8.0770, dtype=torch.float64)\n",
      "14.375338315963745\n",
      "current at batch: 162 tensor(7.4220, dtype=torch.float64)\n",
      "13.79797649383545\n",
      "current at batch: 163 tensor(7.7625, dtype=torch.float64)\n",
      "13.925991296768188\n",
      "current at batch: 164 tensor(7.9158, dtype=torch.float64)\n",
      "13.670850276947021\n",
      "current at batch: 165 tensor(7.5817, dtype=torch.float64)\n",
      "13.915607929229736\n",
      "current at batch: 166 tensor(7.5624, dtype=torch.float64)\n",
      "13.286528825759888\n",
      "current at batch: 167 tensor(7.3004, dtype=torch.float64)\n",
      "14.071313381195068\n",
      "current at batch: 168 tensor(7.1510, dtype=torch.float64)\n",
      "13.585277795791626\n",
      "current at batch: 169 tensor(7.5064, dtype=torch.float64)\n",
      "14.497964143753052\n",
      "current at batch: 170 tensor(7.5141, dtype=torch.float64)\n",
      "14.097736597061157\n",
      "current at batch: 171 tensor(7.8698, dtype=torch.float64)\n",
      "14.886701822280884\n",
      "current at batch: 172 tensor(7.7950, dtype=torch.float64)\n",
      "14.044808626174927\n",
      "current at batch: 173 tensor(7.6487, dtype=torch.float64)\n",
      "14.854679346084595\n",
      "current at batch: 174 tensor(7.5844, dtype=torch.float64)\n",
      "14.709049463272095\n",
      "current at batch: 175 tensor(7.4358, dtype=torch.float64)\n",
      "15.449143648147583\n",
      "current at batch: 176 tensor(7.2247, dtype=torch.float64)\n",
      "14.723666906356812\n",
      "current at batch: 177 tensor(7.4361, dtype=torch.float64)\n",
      "13.931755542755127\n",
      "current at batch: 178 tensor(7.1456, dtype=torch.float64)\n",
      "14.14572787284851\n",
      "current at batch: 179 tensor(7.2011, dtype=torch.float64)\n",
      "14.734310865402222\n",
      "current at batch: 180 tensor(7.5916, dtype=torch.float64)\n",
      "15.165879726409912\n",
      "current at batch: 181 tensor(7.0953, dtype=torch.float64)\n",
      "14.479150533676147\n",
      "current at batch: 182 tensor(7.1753, dtype=torch.float64)\n",
      "14.47464394569397\n",
      "current at batch: 183 tensor(7.8121, dtype=torch.float64)\n",
      "16.098968505859375\n",
      "current at batch: 184 tensor(6.9720, dtype=torch.float64)\n",
      "14.167786836624146\n",
      "current at batch: 185 tensor(7.4847, dtype=torch.float64)\n",
      "15.60815954208374\n",
      "current at batch: 186 tensor(7.5247, dtype=torch.float64)\n",
      "14.942372560501099\n",
      "current at batch: 187 tensor(7.5330, dtype=torch.float64)\n",
      "15.232596397399902\n",
      "current at batch: 188 tensor(7.3841, dtype=torch.float64)\n",
      "15.598817110061646\n",
      "current at batch: 189 tensor(6.7402, dtype=torch.float64)\n",
      "7.02760124206543\n",
      "epoch =  0 \n",
      " tensor(1579.0111, dtype=torch.float64)\n",
      "current at batch: 1 tensor(7.5753, dtype=torch.float64)\n",
      "13.456630945205688\n",
      "current at batch: 2 tensor(7.4357, dtype=torch.float64)\n",
      "13.32014274597168\n",
      "current at batch: 3 tensor(7.1125, dtype=torch.float64)\n",
      "14.98941421508789\n",
      "current at batch: 4 tensor(7.7316, dtype=torch.float64)\n",
      "14.95344614982605\n",
      "current at batch: 5 tensor(7.6735, dtype=torch.float64)\n",
      "15.276610374450684\n",
      "current at batch: 6 tensor(7.3663, dtype=torch.float64)\n",
      "16.37246060371399\n",
      "current at batch: 7 tensor(7.4622, dtype=torch.float64)\n",
      "15.662360668182373\n",
      "current at batch: 8 tensor(7.5448, dtype=torch.float64)\n",
      "16.119234561920166\n",
      "current at batch: 9 tensor(7.1949, dtype=torch.float64)\n",
      "15.777448415756226\n",
      "current at batch: 10 tensor(7.5949, dtype=torch.float64)\n",
      "15.398253917694092\n",
      "current at batch: 11 tensor(7.3305, dtype=torch.float64)\n",
      "14.478536605834961\n",
      "current at batch: 12 tensor(6.9820, dtype=torch.float64)\n",
      "14.560567617416382\n",
      "current at batch: 13 tensor(7.0988, dtype=torch.float64)\n",
      "13.924661874771118\n",
      "current at batch: 14 tensor(7.4436, dtype=torch.float64)\n",
      "15.232146739959717\n",
      "current at batch: 15 tensor(6.8894, dtype=torch.float64)\n",
      "15.002302169799805\n",
      "current at batch: 16 tensor(7.0822, dtype=torch.float64)\n",
      "14.822437047958374\n",
      "current at batch: 17 tensor(7.3329, dtype=torch.float64)\n",
      "16.88156533241272\n",
      "current at batch: 18 tensor(7.4602, dtype=torch.float64)\n",
      "15.818437576293945\n",
      "current at batch: 19 tensor(7.3722, dtype=torch.float64)\n",
      "15.415009021759033\n",
      "current at batch: 20 tensor(6.9985, dtype=torch.float64)\n",
      "14.9634268283844\n",
      "current at batch: 21 tensor(7.2821, dtype=torch.float64)\n",
      "15.43659257888794\n",
      "current at batch: 22 tensor(6.9696, dtype=torch.float64)\n",
      "14.631622552871704\n",
      "current at batch: 23 tensor(6.9910, dtype=torch.float64)\n",
      "13.936540126800537\n",
      "current at batch: 24 tensor(7.2604, dtype=torch.float64)\n",
      "15.439113140106201\n",
      "current at batch: 25 tensor(7.1319, dtype=torch.float64)\n",
      "14.207127571105957\n",
      "current at batch: 26 tensor(7.5305, dtype=torch.float64)\n",
      "14.65658688545227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 27 tensor(7.3136, dtype=torch.float64)\n",
      "14.285166025161743\n",
      "current at batch: 28 tensor(7.0051, dtype=torch.float64)\n",
      "14.491649150848389\n",
      "current at batch: 29 tensor(7.1790, dtype=torch.float64)\n",
      "14.997846603393555\n",
      "current at batch: 30 tensor(7.0105, dtype=torch.float64)\n",
      "14.218158721923828\n",
      "current at batch: 31 tensor(7.2937, dtype=torch.float64)\n",
      "14.856181621551514\n",
      "current at batch: 32 tensor(7.2492, dtype=torch.float64)\n",
      "14.8982834815979\n",
      "current at batch: 33 tensor(7.3680, dtype=torch.float64)\n",
      "14.821025609970093\n",
      "current at batch: 34 tensor(6.9105, dtype=torch.float64)\n",
      "14.287571668624878\n",
      "current at batch: 35 tensor(7.5634, dtype=torch.float64)\n",
      "15.506197214126587\n",
      "current at batch: 36 tensor(6.7610, dtype=torch.float64)\n",
      "14.816304922103882\n",
      "current at batch: 37 tensor(6.8798, dtype=torch.float64)\n",
      "14.901920318603516\n",
      "current at batch: 38 tensor(7.2873, dtype=torch.float64)\n",
      "15.386615753173828\n",
      "current at batch: 39 tensor(6.8257, dtype=torch.float64)\n",
      "14.834191083908081\n",
      "current at batch: 40 tensor(7.3367, dtype=torch.float64)\n",
      "15.40165638923645\n",
      "current at batch: 41 tensor(7.1276, dtype=torch.float64)\n",
      "14.16575813293457\n",
      "current at batch: 42 tensor(7.3240, dtype=torch.float64)\n",
      "15.552235841751099\n",
      "current at batch: 43 tensor(7.2247, dtype=torch.float64)\n",
      "14.86726689338684\n",
      "current at batch: 44 tensor(7.0536, dtype=torch.float64)\n",
      "15.180641174316406\n",
      "current at batch: 45 tensor(7.1155, dtype=torch.float64)\n",
      "14.995864629745483\n",
      "current at batch: 46 tensor(7.2106, dtype=torch.float64)\n",
      "15.492313146591187\n",
      "current at batch: 47 tensor(7.3090, dtype=torch.float64)\n",
      "15.313515901565552\n",
      "current at batch: 48 tensor(7.1775, dtype=torch.float64)\n",
      "14.324321508407593\n",
      "current at batch: 49 tensor(6.9239, dtype=torch.float64)\n",
      "14.884314775466919\n",
      "current at batch: 50 tensor(6.9843, dtype=torch.float64)\n",
      "14.837305068969727\n",
      "current at batch: 51 tensor(6.7636, dtype=torch.float64)\n",
      "14.325884103775024\n",
      "current at batch: 52 tensor(7.4771, dtype=torch.float64)\n",
      "15.942747592926025\n",
      "current at batch: 53 tensor(6.9762, dtype=torch.float64)\n",
      "14.579505443572998\n",
      "current at batch: 54 tensor(7.2757, dtype=torch.float64)\n",
      "15.10817837715149\n",
      "current at batch: 55 tensor(6.9360, dtype=torch.float64)\n",
      "14.42513656616211\n",
      "current at batch: 56 tensor(7.3475, dtype=torch.float64)\n",
      "15.442748546600342\n",
      "current at batch: 57 tensor(7.2224, dtype=torch.float64)\n",
      "14.512009382247925\n",
      "current at batch: 58 tensor(7.0583, dtype=torch.float64)\n",
      "15.545826196670532\n",
      "current at batch: 59 tensor(6.9068, dtype=torch.float64)\n",
      "15.581295251846313\n",
      "current at batch: 60 tensor(7.2404, dtype=torch.float64)\n",
      "15.3574059009552\n",
      "current at batch: 61 tensor(7.3814, dtype=torch.float64)\n",
      "15.523250818252563\n",
      "current at batch: 62 tensor(7.0028, dtype=torch.float64)\n",
      "15.181566953659058\n",
      "current at batch: 63 tensor(6.8318, dtype=torch.float64)\n",
      "14.814985990524292\n",
      "current at batch: 64 tensor(7.1983, dtype=torch.float64)\n",
      "14.779722213745117\n",
      "current at batch: 65 tensor(7.3275, dtype=torch.float64)\n",
      "14.922384262084961\n",
      "current at batch: 66 tensor(6.8148, dtype=torch.float64)\n",
      "15.032448053359985\n",
      "current at batch: 67 tensor(7.1161, dtype=torch.float64)\n",
      "14.6262686252594\n",
      "current at batch: 68 tensor(7.0220, dtype=torch.float64)\n",
      "14.964542627334595\n",
      "current at batch: 69 tensor(6.5420, dtype=torch.float64)\n",
      "14.649564743041992\n",
      "current at batch: 70 tensor(7.3100, dtype=torch.float64)\n",
      "15.285608530044556\n",
      "current at batch: 71 tensor(7.2194, dtype=torch.float64)\n",
      "15.299493312835693\n",
      "current at batch: 72 tensor(7.0551, dtype=torch.float64)\n",
      "15.839302778244019\n",
      "current at batch: 73 tensor(7.6708, dtype=torch.float64)\n",
      "15.113413333892822\n",
      "current at batch: 74 tensor(7.5908, dtype=torch.float64)\n",
      "15.52672553062439\n",
      "current at batch: 75 tensor(6.5825, dtype=torch.float64)\n",
      "13.93109941482544\n",
      "current at batch: 76 tensor(7.1005, dtype=torch.float64)\n",
      "15.178391933441162\n",
      "current at batch: 77 tensor(7.0727, dtype=torch.float64)\n",
      "15.877042293548584\n",
      "current at batch: 78 tensor(7.7531, dtype=torch.float64)\n",
      "15.014438152313232\n",
      "current at batch: 79 tensor(6.7327, dtype=torch.float64)\n",
      "15.201324939727783\n",
      "current at batch: 80 tensor(6.5376, dtype=torch.float64)\n",
      "15.30855107307434\n",
      "current at batch: 81 tensor(7.0764, dtype=torch.float64)\n",
      "15.581300258636475\n",
      "current at batch: 82 tensor(6.8943, dtype=torch.float64)\n",
      "15.664917945861816\n",
      "current at batch: 83 tensor(6.9269, dtype=torch.float64)\n",
      "14.006595849990845\n",
      "current at batch: 84 tensor(7.2505, dtype=torch.float64)\n",
      "14.781152486801147\n",
      "current at batch: 85 tensor(6.5781, dtype=torch.float64)\n",
      "14.467923879623413\n",
      "current at batch: 86 tensor(6.9868, dtype=torch.float64)\n",
      "14.77546501159668\n",
      "current at batch: 87 tensor(6.6347, dtype=torch.float64)\n",
      "14.325309991836548\n",
      "current at batch: 88 tensor(7.0759, dtype=torch.float64)\n",
      "15.678020000457764\n",
      "current at batch: 89 tensor(6.9059, dtype=torch.float64)\n",
      "14.784168004989624\n",
      "current at batch: 90 tensor(6.5810, dtype=torch.float64)\n",
      "15.865442514419556\n",
      "current at batch: 91 tensor(6.9138, dtype=torch.float64)\n",
      "14.774235248565674\n",
      "current at batch: 92 tensor(6.9042, dtype=torch.float64)\n",
      "14.215932369232178\n",
      "current at batch: 93 tensor(6.6622, dtype=torch.float64)\n",
      "14.68420696258545\n",
      "current at batch: 94 tensor(7.0487, dtype=torch.float64)\n",
      "15.559337854385376\n",
      "current at batch: 95 tensor(6.2817, dtype=torch.float64)\n",
      "14.860252380371094\n",
      "current at batch: 96 tensor(6.7691, dtype=torch.float64)\n",
      "15.605298519134521\n",
      "current at batch: 97 tensor(6.8499, dtype=torch.float64)\n",
      "14.818790197372437\n",
      "current at batch: 98 tensor(7.3058, dtype=torch.float64)\n",
      "15.346263885498047\n",
      "current at batch: 99 tensor(7.2324, dtype=torch.float64)\n",
      "14.81276822090149\n",
      "current at batch: 100 tensor(7.0877, dtype=torch.float64)\n",
      "15.405580997467041\n",
      "current at batch: 101 tensor(6.8901, dtype=torch.float64)\n",
      "15.362096548080444\n",
      "current at batch: 102 tensor(6.9623, dtype=torch.float64)\n",
      "15.513338565826416\n",
      "current at batch: 103 tensor(7.0856, dtype=torch.float64)\n",
      "15.267851829528809\n",
      "current at batch: 104 tensor(7.1388, dtype=torch.float64)\n",
      "15.190179586410522\n",
      "current at batch: 105 tensor(6.9921, dtype=torch.float64)\n",
      "15.179226160049438\n",
      "current at batch: 106 tensor(6.8440, dtype=torch.float64)\n",
      "14.588148593902588\n",
      "current at batch: 107 tensor(7.1095, dtype=torch.float64)\n",
      "15.22203278541565\n",
      "current at batch: 108 tensor(6.7887, dtype=torch.float64)\n",
      "14.622910022735596\n",
      "current at batch: 109 tensor(6.8772, dtype=torch.float64)\n",
      "15.17950963973999\n",
      "current at batch: 110 tensor(6.5240, dtype=torch.float64)\n",
      "14.12129020690918\n",
      "current at batch: 111 tensor(6.4672, dtype=torch.float64)\n",
      "15.356366634368896\n",
      "current at batch: 112 tensor(6.9069, dtype=torch.float64)\n",
      "14.711899280548096\n",
      "current at batch: 113 tensor(6.6497, dtype=torch.float64)\n",
      "14.12722635269165\n",
      "current at batch: 114 tensor(6.8041, dtype=torch.float64)\n",
      "14.668203592300415\n",
      "current at batch: 115 tensor(6.7209, dtype=torch.float64)\n",
      "14.639151573181152\n",
      "current at batch: 116 tensor(6.9885, dtype=torch.float64)\n",
      "13.901697874069214\n",
      "current at batch: 117 tensor(6.8470, dtype=torch.float64)\n",
      "15.130856275558472\n",
      "current at batch: 118 tensor(6.8919, dtype=torch.float64)\n",
      "14.264807224273682\n",
      "current at batch: 119 tensor(6.9975, dtype=torch.float64)\n",
      "15.387140989303589\n",
      "current at batch: 120 tensor(6.7463, dtype=torch.float64)\n",
      "14.205743074417114\n",
      "current at batch: 121 tensor(6.9766, dtype=torch.float64)\n",
      "14.695627927780151\n",
      "current at batch: 122 tensor(6.5836, dtype=torch.float64)\n",
      "14.492981672286987\n",
      "current at batch: 123 tensor(6.5659, dtype=torch.float64)\n",
      "15.025404453277588\n",
      "current at batch: 124 tensor(6.7876, dtype=torch.float64)\n",
      "14.585059642791748\n",
      "current at batch: 125 tensor(6.9155, dtype=torch.float64)\n",
      "14.96482539176941\n",
      "current at batch: 126 tensor(6.6938, dtype=torch.float64)\n",
      "14.252443075180054\n",
      "current at batch: 127 tensor(6.9898, dtype=torch.float64)\n",
      "15.435882329940796\n",
      "current at batch: 128 tensor(6.6715, dtype=torch.float64)\n",
      "14.506458520889282\n",
      "current at batch: 129 tensor(6.3732, dtype=torch.float64)\n",
      "14.759160995483398\n",
      "current at batch: 130 tensor(6.6495, dtype=torch.float64)\n",
      "13.75391697883606\n",
      "current at batch: 131 tensor(6.5972, dtype=torch.float64)\n",
      "14.502970695495605\n",
      "current at batch: 132 tensor(6.8037, dtype=torch.float64)\n",
      "14.285796880722046\n",
      "current at batch: 133 tensor(6.2936, dtype=torch.float64)\n",
      "14.775125741958618\n",
      "current at batch: 134 tensor(6.7711, dtype=torch.float64)\n",
      "14.110956192016602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 135 tensor(6.7702, dtype=torch.float64)\n",
      "15.094311475753784\n",
      "current at batch: 136 tensor(6.7530, dtype=torch.float64)\n",
      "14.047097206115723\n",
      "current at batch: 137 tensor(7.0048, dtype=torch.float64)\n",
      "14.187255144119263\n",
      "current at batch: 138 tensor(6.5522, dtype=torch.float64)\n",
      "14.711111545562744\n",
      "current at batch: 139 tensor(6.9451, dtype=torch.float64)\n",
      "15.28750228881836\n",
      "current at batch: 140 tensor(6.7571, dtype=torch.float64)\n",
      "14.71212363243103\n",
      "current at batch: 141 tensor(7.0884, dtype=torch.float64)\n",
      "14.899842023849487\n",
      "current at batch: 142 tensor(6.3734, dtype=torch.float64)\n",
      "13.539773941040039\n",
      "current at batch: 143 tensor(7.0919, dtype=torch.float64)\n",
      "15.683873414993286\n",
      "current at batch: 144 tensor(7.2481, dtype=torch.float64)\n",
      "15.399048089981079\n",
      "current at batch: 145 tensor(6.5021, dtype=torch.float64)\n",
      "14.003620862960815\n",
      "current at batch: 146 tensor(6.6401, dtype=torch.float64)\n",
      "14.638504028320312\n",
      "current at batch: 147 tensor(6.8924, dtype=torch.float64)\n",
      "14.705775499343872\n",
      "current at batch: 148 tensor(6.9060, dtype=torch.float64)\n",
      "15.373474359512329\n",
      "current at batch: 149 tensor(6.6780, dtype=torch.float64)\n",
      "14.444504022598267\n",
      "current at batch: 150 tensor(6.8685, dtype=torch.float64)\n",
      "15.165469408035278\n",
      "current at batch: 151 tensor(6.8566, dtype=torch.float64)\n",
      "15.164081811904907\n",
      "current at batch: 152 tensor(6.3748, dtype=torch.float64)\n",
      "14.222421169281006\n",
      "current at batch: 153 tensor(6.9663, dtype=torch.float64)\n",
      "15.412687540054321\n",
      "current at batch: 154 tensor(6.5894, dtype=torch.float64)\n",
      "14.295907735824585\n",
      "current at batch: 155 tensor(6.9877, dtype=torch.float64)\n",
      "14.906020641326904\n",
      "current at batch: 156 tensor(6.4495, dtype=torch.float64)\n",
      "13.991281032562256\n",
      "current at batch: 157 tensor(6.4190, dtype=torch.float64)\n",
      "13.618052244186401\n",
      "current at batch: 158 tensor(7.0780, dtype=torch.float64)\n",
      "14.766189575195312\n",
      "current at batch: 159 tensor(6.7763, dtype=torch.float64)\n",
      "14.886548519134521\n",
      "current at batch: 160 tensor(6.8792, dtype=torch.float64)\n",
      "14.533843755722046\n",
      "current at batch: 161 tensor(6.1904, dtype=torch.float64)\n",
      "14.672428131103516\n",
      "current at batch: 162 tensor(6.9429, dtype=torch.float64)\n",
      "13.884027481079102\n",
      "current at batch: 163 tensor(6.5364, dtype=torch.float64)\n",
      "15.624399185180664\n",
      "current at batch: 164 tensor(6.7837, dtype=torch.float64)\n",
      "14.39853549003601\n",
      "current at batch: 165 tensor(6.7899, dtype=torch.float64)\n",
      "14.78004264831543\n",
      "current at batch: 166 tensor(6.6326, dtype=torch.float64)\n",
      "14.337939739227295\n",
      "current at batch: 167 tensor(6.7817, dtype=torch.float64)\n",
      "14.7102632522583\n",
      "current at batch: 168 tensor(6.4332, dtype=torch.float64)\n",
      "14.137971639633179\n",
      "current at batch: 169 tensor(6.6370, dtype=torch.float64)\n",
      "14.113791704177856\n",
      "current at batch: 170 tensor(6.6787, dtype=torch.float64)\n",
      "14.224956750869751\n",
      "current at batch: 171 tensor(6.7127, dtype=torch.float64)\n",
      "13.452208042144775\n",
      "current at batch: 172 tensor(6.7866, dtype=torch.float64)\n",
      "14.48264455795288\n",
      "current at batch: 173 tensor(6.7847, dtype=torch.float64)\n",
      "14.92879319190979\n",
      "current at batch: 174 tensor(6.6559, dtype=torch.float64)\n",
      "14.104691743850708\n",
      "current at batch: 175 tensor(6.7826, dtype=torch.float64)\n",
      "13.501882076263428\n",
      "current at batch: 176 tensor(6.5027, dtype=torch.float64)\n",
      "13.538214921951294\n",
      "current at batch: 177 tensor(6.8050, dtype=torch.float64)\n",
      "14.755661487579346\n",
      "current at batch: 178 tensor(7.0858, dtype=torch.float64)\n",
      "15.672808408737183\n",
      "current at batch: 179 tensor(7.3013, dtype=torch.float64)\n",
      "14.091117143630981\n",
      "current at batch: 180 tensor(6.6084, dtype=torch.float64)\n",
      "14.250462055206299\n",
      "current at batch: 181 tensor(6.3247, dtype=torch.float64)\n",
      "14.062165260314941\n",
      "current at batch: 182 tensor(6.9106, dtype=torch.float64)\n",
      "14.752061367034912\n",
      "current at batch: 183 tensor(6.9347, dtype=torch.float64)\n",
      "14.149786949157715\n",
      "current at batch: 184 tensor(6.4030, dtype=torch.float64)\n",
      "14.421377658843994\n",
      "current at batch: 185 tensor(6.8253, dtype=torch.float64)\n",
      "14.298338174819946\n",
      "current at batch: 186 tensor(6.5879, dtype=torch.float64)\n",
      "13.937088966369629\n",
      "current at batch: 187 tensor(6.8737, dtype=torch.float64)\n",
      "15.174397230148315\n",
      "current at batch: 188 tensor(6.7939, dtype=torch.float64)\n",
      "13.855481624603271\n",
      "current at batch: 189 tensor(6.2258, dtype=torch.float64)\n",
      "6.602881193161011\n",
      "epoch =  1 \n",
      " tensor(1313.9786, dtype=torch.float64)\n",
      "current at batch: 1 tensor(6.5077, dtype=torch.float64)\n",
      "14.399517297744751\n",
      "current at batch: 2 tensor(6.0214, dtype=torch.float64)\n",
      "13.669834852218628\n",
      "current at batch: 3 tensor(6.5238, dtype=torch.float64)\n",
      "14.359855890274048\n",
      "current at batch: 4 tensor(6.3607, dtype=torch.float64)\n",
      "14.592570304870605\n",
      "current at batch: 5 tensor(6.4146, dtype=torch.float64)\n",
      "13.584919452667236\n",
      "current at batch: 6 tensor(6.0347, dtype=torch.float64)\n",
      "13.648807525634766\n",
      "current at batch: 7 tensor(6.4994, dtype=torch.float64)\n",
      "14.368344783782959\n",
      "current at batch: 8 tensor(6.3386, dtype=torch.float64)\n",
      "13.29020619392395\n",
      "current at batch: 9 tensor(6.5430, dtype=torch.float64)\n",
      "13.684009552001953\n",
      "current at batch: 10 tensor(6.1859, dtype=torch.float64)\n",
      "13.753604888916016\n",
      "current at batch: 11 tensor(6.5594, dtype=torch.float64)\n",
      "13.682981967926025\n",
      "current at batch: 12 tensor(6.3199, dtype=torch.float64)\n",
      "13.607256412506104\n",
      "current at batch: 13 tensor(6.4618, dtype=torch.float64)\n",
      "13.52132272720337\n",
      "current at batch: 14 tensor(6.2324, dtype=torch.float64)\n",
      "14.029113054275513\n",
      "current at batch: 15 tensor(6.4415, dtype=torch.float64)\n",
      "13.323157787322998\n",
      "current at batch: 16 tensor(6.0500, dtype=torch.float64)\n",
      "14.04696273803711\n",
      "current at batch: 17 tensor(6.7649, dtype=torch.float64)\n",
      "14.594085693359375\n",
      "current at batch: 18 tensor(6.3499, dtype=torch.float64)\n",
      "13.563685178756714\n",
      "current at batch: 19 tensor(5.8620, dtype=torch.float64)\n",
      "13.099447250366211\n",
      "current at batch: 20 tensor(6.0035, dtype=torch.float64)\n",
      "13.455684423446655\n",
      "current at batch: 21 tensor(5.8717, dtype=torch.float64)\n",
      "13.430792808532715\n",
      "current at batch: 22 tensor(6.2628, dtype=torch.float64)\n",
      "13.495487928390503\n",
      "current at batch: 23 tensor(7.0366, dtype=torch.float64)\n",
      "11.942126989364624\n",
      "current at batch: 24 tensor(5.8776, dtype=torch.float64)\n",
      "13.051415920257568\n",
      "current at batch: 25 tensor(6.4083, dtype=torch.float64)\n",
      "13.518452405929565\n",
      "current at batch: 26 tensor(6.7366, dtype=torch.float64)\n",
      "14.451057434082031\n",
      "current at batch: 27 tensor(6.8603, dtype=torch.float64)\n",
      "14.167712926864624\n",
      "current at batch: 28 tensor(6.7464, dtype=torch.float64)\n",
      "13.797136545181274\n",
      "current at batch: 29 tensor(6.4003, dtype=torch.float64)\n",
      "13.424671411514282\n",
      "current at batch: 30 tensor(6.3036, dtype=torch.float64)\n",
      "13.403597593307495\n",
      "current at batch: 31 tensor(6.4067, dtype=torch.float64)\n",
      "13.929410934448242\n",
      "current at batch: 32 tensor(6.3078, dtype=torch.float64)\n",
      "12.765883207321167\n",
      "current at batch: 33 tensor(6.0831, dtype=torch.float64)\n",
      "13.4963960647583\n",
      "current at batch: 34 tensor(6.7455, dtype=torch.float64)\n",
      "14.079264163970947\n",
      "current at batch: 35 tensor(6.2103, dtype=torch.float64)\n",
      "13.635515213012695\n",
      "current at batch: 36 tensor(6.1749, dtype=torch.float64)\n",
      "14.158509254455566\n",
      "current at batch: 37 tensor(6.3467, dtype=torch.float64)\n",
      "13.683434963226318\n",
      "current at batch: 38 tensor(6.1645, dtype=torch.float64)\n",
      "13.1077880859375\n",
      "current at batch: 39 tensor(6.3933, dtype=torch.float64)\n",
      "14.872768640518188\n",
      "current at batch: 40 tensor(6.2639, dtype=torch.float64)\n",
      "13.46879267692566\n",
      "current at batch: 41 tensor(5.9953, dtype=torch.float64)\n",
      "13.166351556777954\n",
      "current at batch: 42 tensor(6.5566, dtype=torch.float64)\n",
      "13.81507921218872\n",
      "current at batch: 43 tensor(6.2335, dtype=torch.float64)\n",
      "11.677974462509155\n",
      "current at batch: 44 tensor(6.5861, dtype=torch.float64)\n",
      "10.887233972549438\n",
      "current at batch: 45 tensor(6.3293, dtype=torch.float64)\n",
      "9.951781272888184\n",
      "current at batch: 46 tensor(6.6093, dtype=torch.float64)\n",
      "10.098824977874756\n",
      "current at batch: 47 tensor(6.6754, dtype=torch.float64)\n",
      "10.781450748443604\n",
      "current at batch: 48 tensor(6.4705, dtype=torch.float64)\n",
      "10.96436882019043\n",
      "current at batch: 49 tensor(6.1597, dtype=torch.float64)\n",
      "10.020263433456421\n",
      "current at batch: 50 tensor(6.6669, dtype=torch.float64)\n",
      "10.009684085845947\n",
      "current at batch: 51 tensor(6.1312, dtype=torch.float64)\n",
      "9.408752918243408\n",
      "current at batch: 52 tensor(6.5766, dtype=torch.float64)\n",
      "9.845093727111816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 53 tensor(6.2258, dtype=torch.float64)\n",
      "9.649425268173218\n",
      "current at batch: 54 tensor(6.1503, dtype=torch.float64)\n",
      "10.385953664779663\n",
      "current at batch: 55 tensor(6.3761, dtype=torch.float64)\n",
      "9.8406343460083\n",
      "current at batch: 56 tensor(6.1852, dtype=torch.float64)\n",
      "10.162376165390015\n",
      "current at batch: 57 tensor(6.6547, dtype=torch.float64)\n",
      "10.035782098770142\n",
      "current at batch: 58 tensor(6.3278, dtype=torch.float64)\n",
      "10.031010627746582\n",
      "current at batch: 59 tensor(6.0891, dtype=torch.float64)\n",
      "9.792986154556274\n",
      "current at batch: 60 tensor(6.2589, dtype=torch.float64)\n",
      "9.75722885131836\n",
      "current at batch: 61 tensor(6.2443, dtype=torch.float64)\n",
      "9.303176403045654\n",
      "current at batch: 62 tensor(6.2098, dtype=torch.float64)\n",
      "10.026819705963135\n",
      "current at batch: 63 tensor(6.1163, dtype=torch.float64)\n",
      "9.306679010391235\n",
      "current at batch: 64 tensor(6.4398, dtype=torch.float64)\n",
      "9.552989721298218\n",
      "current at batch: 65 tensor(6.5523, dtype=torch.float64)\n",
      "10.603610277175903\n",
      "current at batch: 66 tensor(6.3550, dtype=torch.float64)\n",
      "10.254859447479248\n",
      "current at batch: 67 tensor(6.1129, dtype=torch.float64)\n",
      "9.429847955703735\n",
      "current at batch: 68 tensor(6.2221, dtype=torch.float64)\n",
      "10.375945806503296\n",
      "current at batch: 69 tensor(6.5363, dtype=torch.float64)\n",
      "9.898123741149902\n",
      "current at batch: 70 tensor(6.5878, dtype=torch.float64)\n",
      "9.771011590957642\n",
      "current at batch: 71 tensor(6.2465, dtype=torch.float64)\n",
      "9.97455644607544\n",
      "current at batch: 72 tensor(6.1252, dtype=torch.float64)\n",
      "9.663146018981934\n",
      "current at batch: 73 tensor(6.3667, dtype=torch.float64)\n",
      "10.140277862548828\n",
      "current at batch: 74 tensor(6.2765, dtype=torch.float64)\n",
      "9.859103202819824\n",
      "current at batch: 75 tensor(6.5433, dtype=torch.float64)\n",
      "9.812449216842651\n",
      "current at batch: 76 tensor(6.0995, dtype=torch.float64)\n",
      "9.91361141204834\n",
      "current at batch: 77 tensor(6.3299, dtype=torch.float64)\n",
      "9.65450143814087\n",
      "current at batch: 78 tensor(6.1046, dtype=torch.float64)\n",
      "9.970654964447021\n",
      "current at batch: 79 tensor(6.4024, dtype=torch.float64)\n",
      "9.869722604751587\n",
      "current at batch: 80 tensor(6.5737, dtype=torch.float64)\n",
      "10.277953147888184\n",
      "current at batch: 81 tensor(5.9618, dtype=torch.float64)\n",
      "9.983723640441895\n",
      "current at batch: 82 tensor(6.7735, dtype=torch.float64)\n",
      "9.840061902999878\n",
      "current at batch: 83 tensor(6.2590, dtype=torch.float64)\n",
      "9.479324102401733\n",
      "current at batch: 84 tensor(6.6006, dtype=torch.float64)\n",
      "10.424029350280762\n",
      "current at batch: 85 tensor(6.3893, dtype=torch.float64)\n",
      "9.53641963005066\n",
      "current at batch: 86 tensor(6.5162, dtype=torch.float64)\n",
      "10.742223978042603\n",
      "current at batch: 87 tensor(6.4544, dtype=torch.float64)\n",
      "10.016211986541748\n",
      "current at batch: 88 tensor(6.1238, dtype=torch.float64)\n",
      "9.784021139144897\n",
      "current at batch: 89 tensor(6.0840, dtype=torch.float64)\n",
      "10.329485893249512\n",
      "current at batch: 90 tensor(6.7018, dtype=torch.float64)\n",
      "10.129197120666504\n",
      "current at batch: 91 tensor(6.4222, dtype=torch.float64)\n",
      "9.495360374450684\n",
      "current at batch: 92 tensor(6.6005, dtype=torch.float64)\n",
      "10.500080585479736\n",
      "current at batch: 93 tensor(6.4507, dtype=torch.float64)\n",
      "9.704800844192505\n",
      "current at batch: 94 tensor(6.5841, dtype=torch.float64)\n",
      "10.585225820541382\n",
      "current at batch: 95 tensor(6.2092, dtype=torch.float64)\n",
      "9.569390058517456\n",
      "current at batch: 96 tensor(6.3671, dtype=torch.float64)\n",
      "9.538412809371948\n",
      "current at batch: 97 tensor(6.2439, dtype=torch.float64)\n",
      "9.920119047164917\n",
      "current at batch: 98 tensor(6.1952, dtype=torch.float64)\n",
      "9.59938907623291\n",
      "current at batch: 99 tensor(6.1535, dtype=torch.float64)\n",
      "9.826641321182251\n",
      "current at batch: 100 tensor(6.3652, dtype=torch.float64)\n",
      "9.20467734336853\n",
      "current at batch: 101 tensor(6.1258, dtype=torch.float64)\n",
      "10.011183977127075\n",
      "current at batch: 102 tensor(6.4429, dtype=torch.float64)\n",
      "9.986390113830566\n",
      "current at batch: 103 tensor(6.4595, dtype=torch.float64)\n",
      "10.186338186264038\n",
      "current at batch: 104 tensor(6.2895, dtype=torch.float64)\n",
      "9.489360094070435\n",
      "current at batch: 105 tensor(5.9574, dtype=torch.float64)\n",
      "9.77354621887207\n",
      "current at batch: 106 tensor(6.3705, dtype=torch.float64)\n",
      "9.71989893913269\n",
      "current at batch: 107 tensor(6.0781, dtype=torch.float64)\n",
      "9.737732410430908\n",
      "current at batch: 108 tensor(6.5163, dtype=torch.float64)\n",
      "10.450111865997314\n",
      "current at batch: 109 tensor(6.1822, dtype=torch.float64)\n",
      "10.053329467773438\n",
      "current at batch: 110 tensor(6.3261, dtype=torch.float64)\n",
      "10.137339353561401\n",
      "current at batch: 111 tensor(6.1872, dtype=torch.float64)\n",
      "10.081857204437256\n",
      "current at batch: 112 tensor(6.4350, dtype=torch.float64)\n",
      "10.088292360305786\n",
      "current at batch: 113 tensor(6.5362, dtype=torch.float64)\n",
      "9.970767736434937\n",
      "current at batch: 114 tensor(6.3032, dtype=torch.float64)\n",
      "10.053308010101318\n",
      "current at batch: 115 tensor(6.0606, dtype=torch.float64)\n",
      "9.354772806167603\n",
      "current at batch: 116 tensor(6.1692, dtype=torch.float64)\n",
      "9.747741222381592\n",
      "current at batch: 117 tensor(6.4958, dtype=torch.float64)\n",
      "9.200685977935791\n",
      "current at batch: 118 tensor(6.1064, dtype=torch.float64)\n",
      "10.119890928268433\n",
      "current at batch: 119 tensor(6.3287, dtype=torch.float64)\n",
      "9.592445850372314\n",
      "current at batch: 120 tensor(5.9396, dtype=torch.float64)\n",
      "9.313743829727173\n",
      "current at batch: 121 tensor(6.1273, dtype=torch.float64)\n",
      "9.899694442749023\n",
      "current at batch: 122 tensor(6.5371, dtype=torch.float64)\n",
      "9.975722789764404\n",
      "current at batch: 123 tensor(6.2391, dtype=torch.float64)\n",
      "9.165858745574951\n",
      "current at batch: 124 tensor(6.1606, dtype=torch.float64)\n",
      "9.710758447647095\n",
      "current at batch: 125 tensor(6.3463, dtype=torch.float64)\n",
      "9.895388841629028\n",
      "current at batch: 126 tensor(6.4210, dtype=torch.float64)\n",
      "9.713020086288452\n",
      "current at batch: 127 tensor(6.5224, dtype=torch.float64)\n",
      "9.707027673721313\n",
      "current at batch: 128 tensor(6.2501, dtype=torch.float64)\n",
      "9.639237880706787\n",
      "current at batch: 129 tensor(6.5966, dtype=torch.float64)\n",
      "9.924561023712158\n",
      "current at batch: 130 tensor(6.1580, dtype=torch.float64)\n",
      "9.902283668518066\n",
      "current at batch: 131 tensor(6.2343, dtype=torch.float64)\n",
      "9.745165348052979\n",
      "current at batch: 132 tensor(6.6022, dtype=torch.float64)\n",
      "9.613056421279907\n",
      "current at batch: 133 tensor(6.3490, dtype=torch.float64)\n",
      "10.107401847839355\n",
      "current at batch: 134 tensor(6.3963, dtype=torch.float64)\n",
      "10.063788175582886\n",
      "current at batch: 135 tensor(6.1466, dtype=torch.float64)\n",
      "9.603954315185547\n",
      "current at batch: 136 tensor(6.2058, dtype=torch.float64)\n",
      "9.278173685073853\n",
      "current at batch: 137 tensor(5.9001, dtype=torch.float64)\n",
      "8.698742628097534\n",
      "current at batch: 138 tensor(6.3163, dtype=torch.float64)\n",
      "9.847084045410156\n",
      "current at batch: 139 tensor(6.2506, dtype=torch.float64)\n",
      "9.776193380355835\n",
      "current at batch: 140 tensor(6.1939, dtype=torch.float64)\n",
      "9.479435920715332\n",
      "current at batch: 141 tensor(6.3709, dtype=torch.float64)\n",
      "10.313637971878052\n",
      "current at batch: 142 tensor(6.0656, dtype=torch.float64)\n",
      "9.586283206939697\n",
      "current at batch: 143 tensor(6.1753, dtype=torch.float64)\n",
      "9.293259620666504\n",
      "current at batch: 144 tensor(6.2163, dtype=torch.float64)\n",
      "9.956645727157593\n",
      "current at batch: 145 tensor(6.1864, dtype=torch.float64)\n",
      "9.489397048950195\n",
      "current at batch: 146 tensor(6.3286, dtype=torch.float64)\n",
      "9.817045211791992\n",
      "current at batch: 147 tensor(6.3306, dtype=torch.float64)\n",
      "9.915232419967651\n",
      "current at batch: 148 tensor(6.1049, dtype=torch.float64)\n",
      "9.881591796875\n",
      "current at batch: 149 tensor(6.3655, dtype=torch.float64)\n",
      "10.162821054458618\n",
      "current at batch: 150 tensor(6.2435, dtype=torch.float64)\n",
      "9.37525725364685\n",
      "current at batch: 151 tensor(6.0521, dtype=torch.float64)\n",
      "9.370223760604858\n",
      "current at batch: 152 tensor(6.2670, dtype=torch.float64)\n",
      "9.984665870666504\n",
      "current at batch: 153 tensor(6.5712, dtype=torch.float64)\n",
      "10.039512872695923\n",
      "current at batch: 154 tensor(6.2715, dtype=torch.float64)\n",
      "9.424572467803955\n",
      "current at batch: 155 tensor(6.7076, dtype=torch.float64)\n",
      "10.232489824295044\n",
      "current at batch: 156 tensor(5.9323, dtype=torch.float64)\n",
      "10.184809684753418\n",
      "current at batch: 157 tensor(6.2691, dtype=torch.float64)\n",
      "9.644553184509277\n",
      "current at batch: 158 tensor(6.2539, dtype=torch.float64)\n",
      "9.438273429870605\n",
      "current at batch: 159 tensor(6.2788, dtype=torch.float64)\n",
      "9.576483249664307\n",
      "current at batch: 160 tensor(6.1817, dtype=torch.float64)\n",
      "10.452000856399536\n",
      "current at batch: 161 tensor(6.2414, dtype=torch.float64)\n",
      "10.101807594299316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 162 tensor(6.4738, dtype=torch.float64)\n",
      "9.549616575241089\n",
      "current at batch: 163 tensor(6.1796, dtype=torch.float64)\n",
      "9.53287124633789\n",
      "current at batch: 164 tensor(6.2559, dtype=torch.float64)\n",
      "10.218083620071411\n",
      "current at batch: 165 tensor(6.2700, dtype=torch.float64)\n",
      "9.898169994354248\n",
      "current at batch: 166 tensor(6.0162, dtype=torch.float64)\n",
      "9.658300876617432\n",
      "current at batch: 167 tensor(6.1051, dtype=torch.float64)\n",
      "9.710498332977295\n",
      "current at batch: 168 tensor(6.4561, dtype=torch.float64)\n",
      "9.680948257446289\n",
      "current at batch: 169 tensor(6.0653, dtype=torch.float64)\n",
      "9.6324143409729\n",
      "current at batch: 170 tensor(6.1290, dtype=torch.float64)\n",
      "9.647423267364502\n",
      "current at batch: 171 tensor(6.1249, dtype=torch.float64)\n",
      "9.717491149902344\n",
      "current at batch: 172 tensor(6.2923, dtype=torch.float64)\n",
      "9.210609912872314\n",
      "current at batch: 173 tensor(6.4463, dtype=torch.float64)\n",
      "9.690823554992676\n",
      "current at batch: 174 tensor(6.3207, dtype=torch.float64)\n",
      "10.311468601226807\n",
      "current at batch: 175 tensor(6.6609, dtype=torch.float64)\n",
      "10.235846519470215\n",
      "current at batch: 176 tensor(6.2682, dtype=torch.float64)\n",
      "9.88812518119812\n",
      "current at batch: 177 tensor(6.2051, dtype=torch.float64)\n",
      "9.521833419799805\n",
      "current at batch: 178 tensor(6.6818, dtype=torch.float64)\n",
      "9.754160642623901\n",
      "current at batch: 179 tensor(6.5649, dtype=torch.float64)\n",
      "10.11125659942627\n",
      "current at batch: 180 tensor(6.1327, dtype=torch.float64)\n",
      "9.231218099594116\n",
      "current at batch: 181 tensor(6.2912, dtype=torch.float64)\n",
      "9.555376291275024\n",
      "current at batch: 182 tensor(6.2949, dtype=torch.float64)\n",
      "9.911591053009033\n",
      "current at batch: 183 tensor(6.0017, dtype=torch.float64)\n",
      "9.716505527496338\n",
      "current at batch: 184 tensor(6.6522, dtype=torch.float64)\n",
      "10.00472378730774\n",
      "current at batch: 185 tensor(6.0736, dtype=torch.float64)\n",
      "9.476300954818726\n",
      "current at batch: 186 tensor(6.1921, dtype=torch.float64)\n",
      "10.109429597854614\n",
      "current at batch: 187 tensor(6.1913, dtype=torch.float64)\n",
      "9.310181140899658\n",
      "current at batch: 188 tensor(6.1191, dtype=torch.float64)\n",
      "9.584378957748413\n",
      "current at batch: 189 tensor(5.6812, dtype=torch.float64)\n",
      "4.51874303817749\n",
      "epoch =  2 \n",
      " tensor(1193.0588, dtype=torch.float64)\n",
      "current at batch: 1 tensor(5.7616, dtype=torch.float64)\n",
      "8.796368598937988\n",
      "current at batch: 2 tensor(5.8421, dtype=torch.float64)\n",
      "9.537822961807251\n",
      "current at batch: 3 tensor(5.9260, dtype=torch.float64)\n",
      "9.415486574172974\n",
      "current at batch: 4 tensor(5.7521, dtype=torch.float64)\n",
      "9.15537977218628\n",
      "current at batch: 5 tensor(6.0425, dtype=torch.float64)\n",
      "9.324816942214966\n",
      "current at batch: 6 tensor(5.6200, dtype=torch.float64)\n",
      "8.620686769485474\n",
      "current at batch: 7 tensor(6.0522, dtype=torch.float64)\n",
      "9.616172790527344\n",
      "current at batch: 8 tensor(6.2046, dtype=torch.float64)\n",
      "9.196753978729248\n",
      "current at batch: 9 tensor(5.9754, dtype=torch.float64)\n",
      "9.20371389389038\n",
      "current at batch: 10 tensor(6.1122, dtype=torch.float64)\n",
      "9.412869930267334\n",
      "current at batch: 11 tensor(6.0453, dtype=torch.float64)\n",
      "9.42002272605896\n",
      "current at batch: 12 tensor(5.7996, dtype=torch.float64)\n",
      "9.026663064956665\n",
      "current at batch: 13 tensor(6.0543, dtype=torch.float64)\n",
      "8.798015117645264\n",
      "current at batch: 14 tensor(5.9185, dtype=torch.float64)\n",
      "9.489849328994751\n",
      "current at batch: 15 tensor(6.4000, dtype=torch.float64)\n",
      "9.54013180732727\n",
      "current at batch: 16 tensor(5.8158, dtype=torch.float64)\n",
      "9.143120527267456\n",
      "current at batch: 17 tensor(6.1030, dtype=torch.float64)\n",
      "9.551540851593018\n",
      "current at batch: 18 tensor(6.3372, dtype=torch.float64)\n",
      "10.315513610839844\n",
      "current at batch: 19 tensor(6.1507, dtype=torch.float64)\n",
      "9.0070219039917\n",
      "current at batch: 20 tensor(6.2377, dtype=torch.float64)\n",
      "9.382434368133545\n",
      "current at batch: 21 tensor(6.1565, dtype=torch.float64)\n",
      "9.571954727172852\n",
      "current at batch: 22 tensor(5.8721, dtype=torch.float64)\n",
      "9.1746666431427\n",
      "current at batch: 23 tensor(6.0037, dtype=torch.float64)\n",
      "9.909797191619873\n",
      "current at batch: 24 tensor(6.0311, dtype=torch.float64)\n",
      "9.70152473449707\n",
      "current at batch: 25 tensor(5.9558, dtype=torch.float64)\n",
      "8.996519088745117\n",
      "current at batch: 26 tensor(5.8205, dtype=torch.float64)\n",
      "9.22420883178711\n",
      "current at batch: 27 tensor(6.1606, dtype=torch.float64)\n",
      "9.013909101486206\n",
      "current at batch: 28 tensor(6.3744, dtype=torch.float64)\n",
      "9.59646487236023\n",
      "current at batch: 29 tensor(5.7001, dtype=torch.float64)\n",
      "9.016247987747192\n",
      "current at batch: 30 tensor(6.4481, dtype=torch.float64)\n",
      "9.162719488143921\n",
      "current at batch: 31 tensor(5.9083, dtype=torch.float64)\n",
      "9.412315368652344\n",
      "current at batch: 32 tensor(6.2255, dtype=torch.float64)\n",
      "9.279764413833618\n",
      "current at batch: 33 tensor(6.0827, dtype=torch.float64)\n",
      "9.181145906448364\n",
      "current at batch: 34 tensor(5.9427, dtype=torch.float64)\n",
      "9.45541763305664\n",
      "current at batch: 35 tensor(6.1562, dtype=torch.float64)\n",
      "8.977031469345093\n",
      "current at batch: 36 tensor(5.7017, dtype=torch.float64)\n",
      "8.899483919143677\n",
      "current at batch: 37 tensor(5.9096, dtype=torch.float64)\n",
      "9.558452129364014\n",
      "current at batch: 38 tensor(6.2621, dtype=torch.float64)\n",
      "9.292157649993896\n",
      "current at batch: 39 tensor(6.0425, dtype=torch.float64)\n",
      "9.85878849029541\n",
      "current at batch: 40 tensor(5.7889, dtype=torch.float64)\n",
      "9.000396251678467\n",
      "current at batch: 41 tensor(5.8540, dtype=torch.float64)\n",
      "9.310692071914673\n",
      "current at batch: 42 tensor(6.0601, dtype=torch.float64)\n",
      "9.26367998123169\n",
      "current at batch: 43 tensor(5.6969, dtype=torch.float64)\n",
      "9.737488508224487\n",
      "current at batch: 44 tensor(5.8770, dtype=torch.float64)\n",
      "9.138108253479004\n",
      "current at batch: 45 tensor(5.9384, dtype=torch.float64)\n",
      "10.20004153251648\n",
      "current at batch: 46 tensor(5.8324, dtype=torch.float64)\n",
      "9.137057542800903\n",
      "current at batch: 47 tensor(6.0326, dtype=torch.float64)\n",
      "9.643956899642944\n",
      "current at batch: 48 tensor(5.8647, dtype=torch.float64)\n",
      "9.553878545761108\n",
      "current at batch: 49 tensor(5.5689, dtype=torch.float64)\n",
      "8.96743655204773\n",
      "current at batch: 50 tensor(6.0283, dtype=torch.float64)\n",
      "9.490811109542847\n",
      "current at batch: 51 tensor(5.9508, dtype=torch.float64)\n",
      "9.467849493026733\n",
      "current at batch: 52 tensor(6.2573, dtype=torch.float64)\n",
      "8.972800493240356\n",
      "current at batch: 53 tensor(6.2451, dtype=torch.float64)\n",
      "9.783550500869751\n",
      "current at batch: 54 tensor(5.7659, dtype=torch.float64)\n",
      "9.205606460571289\n",
      "current at batch: 55 tensor(6.0971, dtype=torch.float64)\n",
      "9.774014711380005\n",
      "current at batch: 56 tensor(6.1406, dtype=torch.float64)\n",
      "9.643572330474854\n",
      "current at batch: 57 tensor(5.9899, dtype=torch.float64)\n",
      "9.522354364395142\n",
      "current at batch: 58 tensor(6.1855, dtype=torch.float64)\n",
      "9.247166633605957\n",
      "current at batch: 59 tensor(6.1866, dtype=torch.float64)\n",
      "9.398803472518921\n",
      "current at batch: 60 tensor(6.2036, dtype=torch.float64)\n",
      "9.375235319137573\n",
      "current at batch: 61 tensor(6.0169, dtype=torch.float64)\n",
      "9.554075956344604\n",
      "current at batch: 62 tensor(6.1459, dtype=torch.float64)\n",
      "10.447075366973877\n",
      "current at batch: 63 tensor(5.9443, dtype=torch.float64)\n",
      "9.075012683868408\n",
      "current at batch: 64 tensor(6.0661, dtype=torch.float64)\n",
      "9.146000146865845\n",
      "current at batch: 65 tensor(6.0943, dtype=torch.float64)\n",
      "9.284192085266113\n",
      "current at batch: 66 tensor(6.0519, dtype=torch.float64)\n",
      "9.691658020019531\n",
      "current at batch: 67 tensor(5.9740, dtype=torch.float64)\n",
      "9.252163171768188\n",
      "current at batch: 68 tensor(6.0280, dtype=torch.float64)\n",
      "9.405012845993042\n",
      "current at batch: 69 tensor(5.9516, dtype=torch.float64)\n",
      "9.54599642753601\n",
      "current at batch: 70 tensor(6.1267, dtype=torch.float64)\n",
      "9.647445440292358\n",
      "current at batch: 71 tensor(5.9270, dtype=torch.float64)\n",
      "9.618557453155518\n",
      "current at batch: 72 tensor(6.2123, dtype=torch.float64)\n",
      "9.722039699554443\n",
      "current at batch: 73 tensor(5.9201, dtype=torch.float64)\n",
      "9.563924789428711\n",
      "current at batch: 74 tensor(5.9044, dtype=torch.float64)\n",
      "8.927474975585938\n",
      "current at batch: 75 tensor(6.1999, dtype=torch.float64)\n",
      "9.79460859298706\n",
      "current at batch: 76 tensor(5.9681, dtype=torch.float64)\n",
      "9.377183675765991\n",
      "current at batch: 77 tensor(5.7671, dtype=torch.float64)\n",
      "9.573774337768555\n",
      "current at batch: 78 tensor(5.8228, dtype=torch.float64)\n",
      "9.224586963653564\n",
      "current at batch: 79 tensor(5.8146, dtype=torch.float64)\n",
      "9.41733431816101\n",
      "current at batch: 80 tensor(5.8689, dtype=torch.float64)\n",
      "9.105093240737915\n",
      "current at batch: 81 tensor(6.3880, dtype=torch.float64)\n",
      "9.768159627914429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 82 tensor(6.3873, dtype=torch.float64)\n",
      "9.668160676956177\n",
      "current at batch: 83 tensor(6.1119, dtype=torch.float64)\n",
      "9.425824880599976\n",
      "current at batch: 84 tensor(5.8284, dtype=torch.float64)\n",
      "9.407700300216675\n",
      "current at batch: 85 tensor(5.9146, dtype=torch.float64)\n",
      "9.499405145645142\n",
      "current at batch: 86 tensor(6.1221, dtype=torch.float64)\n",
      "9.476565599441528\n",
      "current at batch: 87 tensor(5.8318, dtype=torch.float64)\n",
      "9.736573457717896\n",
      "current at batch: 88 tensor(6.0865, dtype=torch.float64)\n",
      "9.445839643478394\n",
      "current at batch: 89 tensor(5.8883, dtype=torch.float64)\n",
      "9.880191087722778\n",
      "current at batch: 90 tensor(6.2400, dtype=torch.float64)\n",
      "9.717550992965698\n",
      "current at batch: 91 tensor(5.9955, dtype=torch.float64)\n",
      "9.946728229522705\n",
      "current at batch: 92 tensor(6.0481, dtype=torch.float64)\n",
      "9.560040473937988\n",
      "current at batch: 93 tensor(5.7999, dtype=torch.float64)\n",
      "9.08611011505127\n",
      "current at batch: 94 tensor(5.6852, dtype=torch.float64)\n",
      "9.597450971603394\n",
      "current at batch: 95 tensor(5.9352, dtype=torch.float64)\n",
      "9.479894399642944\n",
      "current at batch: 96 tensor(6.0986, dtype=torch.float64)\n",
      "9.381713151931763\n",
      "current at batch: 97 tensor(6.2276, dtype=torch.float64)\n",
      "9.638579607009888\n",
      "current at batch: 98 tensor(5.9194, dtype=torch.float64)\n",
      "9.11309814453125\n",
      "current at batch: 99 tensor(6.0061, dtype=torch.float64)\n",
      "9.427189826965332\n",
      "current at batch: 100 tensor(6.1286, dtype=torch.float64)\n",
      "9.671027421951294\n",
      "current at batch: 101 tensor(6.0304, dtype=torch.float64)\n",
      "9.784430503845215\n",
      "current at batch: 102 tensor(5.8986, dtype=torch.float64)\n",
      "9.237882614135742\n",
      "current at batch: 103 tensor(6.2961, dtype=torch.float64)\n",
      "9.26019549369812\n",
      "current at batch: 104 tensor(5.9728, dtype=torch.float64)\n",
      "9.769011497497559\n",
      "current at batch: 105 tensor(5.7914, dtype=torch.float64)\n",
      "9.100531339645386\n",
      "current at batch: 106 tensor(5.7826, dtype=torch.float64)\n",
      "8.9529447555542\n",
      "current at batch: 107 tensor(6.1404, dtype=torch.float64)\n",
      "9.583401679992676\n",
      "current at batch: 108 tensor(5.9978, dtype=torch.float64)\n",
      "9.638431310653687\n",
      "current at batch: 109 tensor(6.2259, dtype=torch.float64)\n",
      "8.755661964416504\n",
      "current at batch: 110 tensor(6.0718, dtype=torch.float64)\n",
      "9.26314663887024\n",
      "current at batch: 111 tensor(5.9306, dtype=torch.float64)\n",
      "8.703770399093628\n",
      "current at batch: 112 tensor(6.0492, dtype=torch.float64)\n",
      "9.395843267440796\n",
      "current at batch: 113 tensor(6.1103, dtype=torch.float64)\n",
      "9.261677503585815\n",
      "current at batch: 114 tensor(6.0488, dtype=torch.float64)\n",
      "9.837563037872314\n",
      "current at batch: 115 tensor(5.7554, dtype=torch.float64)\n",
      "9.02647876739502\n",
      "current at batch: 116 tensor(5.6780, dtype=torch.float64)\n",
      "8.933440685272217\n",
      "current at batch: 117 tensor(6.1829, dtype=torch.float64)\n",
      "9.425763845443726\n",
      "current at batch: 118 tensor(6.0627, dtype=torch.float64)\n",
      "9.283883571624756\n",
      "current at batch: 119 tensor(6.1513, dtype=torch.float64)\n",
      "9.579389095306396\n",
      "current at batch: 120 tensor(6.1508, dtype=torch.float64)\n",
      "9.321187734603882\n",
      "current at batch: 121 tensor(5.7974, dtype=torch.float64)\n",
      "9.662467002868652\n",
      "current at batch: 122 tensor(6.2951, dtype=torch.float64)\n",
      "9.496816396713257\n",
      "current at batch: 123 tensor(6.1979, dtype=torch.float64)\n",
      "9.105034351348877\n",
      "current at batch: 124 tensor(6.3027, dtype=torch.float64)\n",
      "10.049712419509888\n",
      "current at batch: 125 tensor(5.7043, dtype=torch.float64)\n",
      "9.348210096359253\n",
      "current at batch: 126 tensor(6.1407, dtype=torch.float64)\n",
      "9.722124576568604\n",
      "current at batch: 127 tensor(6.2536, dtype=torch.float64)\n",
      "9.81979775428772\n",
      "current at batch: 128 tensor(5.8775, dtype=torch.float64)\n",
      "9.387855052947998\n",
      "current at batch: 129 tensor(6.0807, dtype=torch.float64)\n",
      "9.505821704864502\n",
      "current at batch: 130 tensor(6.2539, dtype=torch.float64)\n",
      "9.410298824310303\n",
      "current at batch: 131 tensor(5.8535, dtype=torch.float64)\n",
      "8.692753791809082\n",
      "current at batch: 132 tensor(5.8369, dtype=torch.float64)\n",
      "9.834028720855713\n",
      "current at batch: 133 tensor(6.0641, dtype=torch.float64)\n",
      "9.362856149673462\n",
      "current at batch: 134 tensor(6.1465, dtype=torch.float64)\n",
      "9.19460654258728\n",
      "current at batch: 135 tensor(5.7862, dtype=torch.float64)\n",
      "9.331696510314941\n",
      "current at batch: 136 tensor(5.9355, dtype=torch.float64)\n",
      "9.63941764831543\n",
      "current at batch: 137 tensor(5.9789, dtype=torch.float64)\n",
      "9.254141330718994\n",
      "current at batch: 138 tensor(6.1518, dtype=torch.float64)\n",
      "9.683263540267944\n",
      "current at batch: 139 tensor(6.0339, dtype=torch.float64)\n",
      "9.574371576309204\n",
      "current at batch: 140 tensor(6.0235, dtype=torch.float64)\n",
      "9.723510980606079\n",
      "current at batch: 141 tensor(5.8767, dtype=torch.float64)\n",
      "9.435294389724731\n",
      "current at batch: 142 tensor(5.9273, dtype=torch.float64)\n",
      "9.595909833908081\n",
      "current at batch: 143 tensor(6.6363, dtype=torch.float64)\n",
      "9.676444292068481\n",
      "current at batch: 144 tensor(5.9787, dtype=torch.float64)\n",
      "9.300174236297607\n",
      "current at batch: 145 tensor(5.9518, dtype=torch.float64)\n",
      "9.684950113296509\n",
      "current at batch: 146 tensor(5.9388, dtype=torch.float64)\n",
      "9.92225193977356\n",
      "current at batch: 147 tensor(6.3359, dtype=torch.float64)\n",
      "9.813128232955933\n",
      "current at batch: 148 tensor(6.3293, dtype=torch.float64)\n",
      "9.4076087474823\n",
      "current at batch: 149 tensor(5.9226, dtype=torch.float64)\n",
      "9.205258846282959\n",
      "current at batch: 150 tensor(5.9582, dtype=torch.float64)\n",
      "9.084542989730835\n",
      "current at batch: 151 tensor(6.1159, dtype=torch.float64)\n",
      "9.84347677230835\n",
      "current at batch: 152 tensor(5.9201, dtype=torch.float64)\n",
      "9.429835557937622\n",
      "current at batch: 153 tensor(6.2302, dtype=torch.float64)\n",
      "9.235202312469482\n",
      "current at batch: 154 tensor(6.1284, dtype=torch.float64)\n",
      "9.270769119262695\n",
      "current at batch: 155 tensor(5.8273, dtype=torch.float64)\n",
      "9.487576007843018\n",
      "current at batch: 156 tensor(6.0964, dtype=torch.float64)\n",
      "9.71952509880066\n",
      "current at batch: 157 tensor(5.7257, dtype=torch.float64)\n",
      "9.229673147201538\n",
      "current at batch: 158 tensor(5.8481, dtype=torch.float64)\n",
      "9.772480249404907\n",
      "current at batch: 159 tensor(6.1239, dtype=torch.float64)\n",
      "9.715000867843628\n",
      "current at batch: 160 tensor(5.9510, dtype=torch.float64)\n",
      "9.545365571975708\n",
      "current at batch: 161 tensor(6.1091, dtype=torch.float64)\n",
      "8.993454456329346\n",
      "current at batch: 162 tensor(5.7062, dtype=torch.float64)\n",
      "8.909894466400146\n",
      "current at batch: 163 tensor(5.6351, dtype=torch.float64)\n",
      "9.199987411499023\n",
      "current at batch: 164 tensor(5.9708, dtype=torch.float64)\n",
      "9.154629945755005\n",
      "current at batch: 165 tensor(5.9646, dtype=torch.float64)\n",
      "9.088663578033447\n",
      "current at batch: 166 tensor(6.0405, dtype=torch.float64)\n",
      "9.36236310005188\n",
      "current at batch: 167 tensor(5.9913, dtype=torch.float64)\n",
      "8.890451908111572\n",
      "current at batch: 168 tensor(6.0368, dtype=torch.float64)\n",
      "9.83366060256958\n",
      "current at batch: 169 tensor(5.6977, dtype=torch.float64)\n",
      "9.061061143875122\n",
      "current at batch: 170 tensor(5.7033, dtype=torch.float64)\n",
      "9.858207941055298\n",
      "current at batch: 171 tensor(6.1231, dtype=torch.float64)\n",
      "9.782517671585083\n",
      "current at batch: 172 tensor(5.6818, dtype=torch.float64)\n",
      "9.090625286102295\n",
      "current at batch: 173 tensor(6.0114, dtype=torch.float64)\n",
      "9.465897798538208\n",
      "current at batch: 174 tensor(6.1316, dtype=torch.float64)\n",
      "10.28682279586792\n",
      "current at batch: 175 tensor(5.7289, dtype=torch.float64)\n",
      "8.970293760299683\n",
      "current at batch: 176 tensor(6.1867, dtype=torch.float64)\n",
      "9.315569162368774\n",
      "current at batch: 177 tensor(6.3606, dtype=torch.float64)\n",
      "9.714235782623291\n",
      "current at batch: 178 tensor(5.9946, dtype=torch.float64)\n",
      "9.324851036071777\n",
      "current at batch: 179 tensor(5.9352, dtype=torch.float64)\n",
      "8.894673824310303\n",
      "current at batch: 180 tensor(5.8210, dtype=torch.float64)\n",
      "9.803312540054321\n",
      "current at batch: 181 tensor(6.3371, dtype=torch.float64)\n",
      "9.106731414794922\n",
      "current at batch: 182 tensor(6.1297, dtype=torch.float64)\n",
      "9.641995906829834\n",
      "current at batch: 183 tensor(6.2248, dtype=torch.float64)\n",
      "9.198962211608887\n",
      "current at batch: 184 tensor(6.0687, dtype=torch.float64)\n",
      "9.392874717712402\n",
      "current at batch: 185 tensor(6.0035, dtype=torch.float64)\n",
      "9.658705472946167\n",
      "current at batch: 186 tensor(5.7984, dtype=torch.float64)\n",
      "8.83248496055603\n",
      "current at batch: 187 tensor(5.9915, dtype=torch.float64)\n",
      "8.778446912765503\n",
      "current at batch: 188 tensor(5.9661, dtype=torch.float64)\n",
      "9.315849542617798\n",
      "current at batch: 189 tensor(5.5062, dtype=torch.float64)\n",
      "4.02891731262207\n",
      "epoch =  3 \n",
      " tensor(1135.6383, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 1 tensor(5.9226, dtype=torch.float64)\n",
      "8.705190896987915\n",
      "current at batch: 2 tensor(6.3454, dtype=torch.float64)\n",
      "9.367782592773438\n",
      "current at batch: 3 tensor(5.5841, dtype=torch.float64)\n",
      "8.868853330612183\n",
      "current at batch: 4 tensor(5.9663, dtype=torch.float64)\n",
      "9.157572746276855\n",
      "current at batch: 5 tensor(5.9384, dtype=torch.float64)\n",
      "8.616909265518188\n",
      "current at batch: 6 tensor(5.6789, dtype=torch.float64)\n",
      "9.238188028335571\n",
      "current at batch: 7 tensor(6.1072, dtype=torch.float64)\n",
      "9.155070066452026\n",
      "current at batch: 8 tensor(5.6815, dtype=torch.float64)\n",
      "9.000986576080322\n",
      "current at batch: 9 tensor(6.2082, dtype=torch.float64)\n",
      "9.482608556747437\n",
      "current at batch: 10 tensor(5.7365, dtype=torch.float64)\n",
      "8.757785081863403\n",
      "current at batch: 11 tensor(5.5670, dtype=torch.float64)\n",
      "9.161575078964233\n",
      "current at batch: 12 tensor(5.4634, dtype=torch.float64)\n",
      "9.160714387893677\n",
      "current at batch: 13 tensor(5.5704, dtype=torch.float64)\n",
      "8.851900339126587\n",
      "current at batch: 14 tensor(6.1222, dtype=torch.float64)\n",
      "9.290013790130615\n",
      "current at batch: 15 tensor(6.0507, dtype=torch.float64)\n",
      "9.47780179977417\n",
      "current at batch: 16 tensor(5.4524, dtype=torch.float64)\n",
      "8.857306718826294\n",
      "current at batch: 17 tensor(5.9998, dtype=torch.float64)\n",
      "9.641282558441162\n",
      "current at batch: 18 tensor(5.7928, dtype=torch.float64)\n",
      "8.92690658569336\n",
      "current at batch: 19 tensor(5.6272, dtype=torch.float64)\n",
      "9.291263341903687\n",
      "current at batch: 20 tensor(5.4109, dtype=torch.float64)\n",
      "8.65976881980896\n",
      "current at batch: 21 tensor(5.7333, dtype=torch.float64)\n",
      "9.452283143997192\n",
      "current at batch: 22 tensor(5.7524, dtype=torch.float64)\n",
      "9.357272386550903\n",
      "current at batch: 23 tensor(5.8740, dtype=torch.float64)\n",
      "8.840344905853271\n",
      "current at batch: 24 tensor(5.7740, dtype=torch.float64)\n",
      "8.94553017616272\n",
      "current at batch: 25 tensor(5.6879, dtype=torch.float64)\n",
      "9.160509824752808\n",
      "current at batch: 26 tensor(5.7753, dtype=torch.float64)\n",
      "8.848590850830078\n",
      "current at batch: 27 tensor(5.8479, dtype=torch.float64)\n",
      "9.327766180038452\n",
      "current at batch: 28 tensor(5.6137, dtype=torch.float64)\n",
      "8.534688472747803\n",
      "current at batch: 29 tensor(6.0782, dtype=torch.float64)\n",
      "9.158572435379028\n",
      "current at batch: 30 tensor(5.4749, dtype=torch.float64)\n",
      "8.832876682281494\n",
      "current at batch: 31 tensor(5.6271, dtype=torch.float64)\n",
      "9.249669551849365\n",
      "current at batch: 32 tensor(5.8882, dtype=torch.float64)\n",
      "8.680442094802856\n",
      "current at batch: 33 tensor(5.8125, dtype=torch.float64)\n",
      "8.835576295852661\n",
      "current at batch: 34 tensor(5.6015, dtype=torch.float64)\n",
      "8.820839881896973\n",
      "current at batch: 35 tensor(5.6786, dtype=torch.float64)\n",
      "9.260260105133057\n",
      "current at batch: 36 tensor(5.7337, dtype=torch.float64)\n",
      "8.708250045776367\n",
      "current at batch: 37 tensor(6.1530, dtype=torch.float64)\n",
      "9.02797818183899\n",
      "current at batch: 38 tensor(5.9919, dtype=torch.float64)\n",
      "9.254012823104858\n",
      "current at batch: 39 tensor(5.5260, dtype=torch.float64)\n",
      "8.98449444770813\n",
      "current at batch: 40 tensor(5.8047, dtype=torch.float64)\n",
      "8.764289855957031\n",
      "current at batch: 41 tensor(5.8537, dtype=torch.float64)\n",
      "9.122619867324829\n",
      "current at batch: 42 tensor(5.7720, dtype=torch.float64)\n",
      "8.88837742805481\n",
      "current at batch: 43 tensor(5.6720, dtype=torch.float64)\n",
      "8.931429862976074\n",
      "current at batch: 44 tensor(5.6820, dtype=torch.float64)\n",
      "9.0250563621521\n",
      "current at batch: 45 tensor(6.2175, dtype=torch.float64)\n",
      "9.182589769363403\n",
      "current at batch: 46 tensor(5.5717, dtype=torch.float64)\n",
      "8.917399883270264\n",
      "current at batch: 47 tensor(5.6541, dtype=torch.float64)\n",
      "8.828887939453125\n",
      "current at batch: 48 tensor(6.2288, dtype=torch.float64)\n",
      "9.484807014465332\n",
      "current at batch: 49 tensor(5.8966, dtype=torch.float64)\n",
      "8.410378217697144\n",
      "current at batch: 50 tensor(5.8148, dtype=torch.float64)\n",
      "8.52511739730835\n",
      "current at batch: 51 tensor(5.5896, dtype=torch.float64)\n",
      "8.945214748382568\n",
      "current at batch: 52 tensor(5.8943, dtype=torch.float64)\n",
      "8.885984659194946\n",
      "current at batch: 53 tensor(5.4892, dtype=torch.float64)\n",
      "8.677254438400269\n",
      "current at batch: 54 tensor(6.0199, dtype=torch.float64)\n",
      "9.250138759613037\n",
      "current at batch: 55 tensor(5.8965, dtype=torch.float64)\n",
      "9.098539590835571\n",
      "current at batch: 56 tensor(5.8400, dtype=torch.float64)\n",
      "9.198206901550293\n",
      "current at batch: 57 tensor(5.7311, dtype=torch.float64)\n",
      "9.144063711166382\n",
      "current at batch: 58 tensor(5.9330, dtype=torch.float64)\n",
      "9.024659156799316\n",
      "current at batch: 59 tensor(5.8654, dtype=torch.float64)\n",
      "9.10525918006897\n",
      "current at batch: 60 tensor(5.9331, dtype=torch.float64)\n",
      "9.341251850128174\n",
      "current at batch: 61 tensor(5.6829, dtype=torch.float64)\n",
      "9.095656633377075\n",
      "current at batch: 62 tensor(6.0909, dtype=torch.float64)\n",
      "9.104667901992798\n",
      "current at batch: 63 tensor(5.6140, dtype=torch.float64)\n",
      "8.44205927848816\n",
      "current at batch: 64 tensor(5.8295, dtype=torch.float64)\n",
      "9.649795532226562\n",
      "current at batch: 65 tensor(5.6505, dtype=torch.float64)\n",
      "8.47458267211914\n",
      "current at batch: 66 tensor(5.8622, dtype=torch.float64)\n",
      "9.189671754837036\n",
      "current at batch: 67 tensor(5.8313, dtype=torch.float64)\n",
      "9.02347707748413\n",
      "current at batch: 68 tensor(5.9624, dtype=torch.float64)\n",
      "9.18215298652649\n",
      "current at batch: 69 tensor(5.6002, dtype=torch.float64)\n",
      "9.095026731491089\n",
      "current at batch: 70 tensor(5.8118, dtype=torch.float64)\n",
      "9.314725875854492\n",
      "current at batch: 71 tensor(5.7855, dtype=torch.float64)\n",
      "9.05556344985962\n",
      "current at batch: 72 tensor(5.7914, dtype=torch.float64)\n",
      "8.893020629882812\n",
      "current at batch: 73 tensor(6.0847, dtype=torch.float64)\n",
      "8.931990623474121\n",
      "current at batch: 74 tensor(5.4582, dtype=torch.float64)\n",
      "8.84898066520691\n",
      "current at batch: 75 tensor(5.8049, dtype=torch.float64)\n",
      "8.891014099121094\n",
      "current at batch: 76 tensor(5.9179, dtype=torch.float64)\n",
      "8.931491613388062\n",
      "current at batch: 77 tensor(5.7479, dtype=torch.float64)\n",
      "9.250849485397339\n",
      "current at batch: 78 tensor(5.4543, dtype=torch.float64)\n",
      "8.9333336353302\n",
      "current at batch: 79 tensor(5.6567, dtype=torch.float64)\n",
      "9.185654163360596\n",
      "current at batch: 80 tensor(5.8104, dtype=torch.float64)\n",
      "9.294955015182495\n",
      "current at batch: 81 tensor(5.5626, dtype=torch.float64)\n",
      "9.511387348175049\n",
      "current at batch: 82 tensor(5.6837, dtype=torch.float64)\n",
      "8.969995021820068\n",
      "current at batch: 83 tensor(5.4234, dtype=torch.float64)\n",
      "8.732174634933472\n",
      "current at batch: 84 tensor(5.4863, dtype=torch.float64)\n",
      "9.068972826004028\n",
      "current at batch: 85 tensor(5.9351, dtype=torch.float64)\n",
      "8.854467153549194\n",
      "current at batch: 86 tensor(5.5639, dtype=torch.float64)\n",
      "9.230981826782227\n",
      "current at batch: 87 tensor(5.8295, dtype=torch.float64)\n",
      "9.23178243637085\n",
      "current at batch: 88 tensor(5.6073, dtype=torch.float64)\n",
      "9.355299472808838\n",
      "current at batch: 89 tensor(5.6861, dtype=torch.float64)\n",
      "8.574793100357056\n",
      "current at batch: 90 tensor(5.9933, dtype=torch.float64)\n",
      "9.388567447662354\n",
      "current at batch: 91 tensor(5.5953, dtype=torch.float64)\n",
      "9.06009578704834\n",
      "current at batch: 92 tensor(5.5895, dtype=torch.float64)\n",
      "9.168884754180908\n",
      "current at batch: 93 tensor(5.8388, dtype=torch.float64)\n",
      "8.983678817749023\n",
      "current at batch: 94 tensor(5.7561, dtype=torch.float64)\n",
      "8.557411432266235\n",
      "current at batch: 95 tensor(5.5208, dtype=torch.float64)\n",
      "8.84540319442749\n",
      "current at batch: 96 tensor(6.1149, dtype=torch.float64)\n",
      "8.811441659927368\n",
      "current at batch: 97 tensor(5.7218, dtype=torch.float64)\n",
      "9.107659816741943\n",
      "current at batch: 98 tensor(5.7982, dtype=torch.float64)\n",
      "9.272607326507568\n",
      "current at batch: 99 tensor(5.8172, dtype=torch.float64)\n",
      "8.556244373321533\n",
      "current at batch: 100 tensor(6.0571, dtype=torch.float64)\n",
      "9.03003740310669\n",
      "current at batch: 101 tensor(6.0853, dtype=torch.float64)\n",
      "9.26823115348816\n",
      "current at batch: 102 tensor(5.7810, dtype=torch.float64)\n",
      "8.763439655303955\n",
      "current at batch: 103 tensor(5.8559, dtype=torch.float64)\n",
      "9.199447631835938\n",
      "current at batch: 104 tensor(5.5627, dtype=torch.float64)\n",
      "8.459085702896118\n",
      "current at batch: 105 tensor(5.6945, dtype=torch.float64)\n",
      "9.108593702316284\n",
      "current at batch: 106 tensor(5.9421, dtype=torch.float64)\n",
      "8.932668447494507\n",
      "current at batch: 107 tensor(5.7764, dtype=torch.float64)\n",
      "8.853353261947632\n",
      "current at batch: 108 tensor(5.2530, dtype=torch.float64)\n",
      "8.984480381011963\n",
      "current at batch: 109 tensor(5.6003, dtype=torch.float64)\n",
      "8.691969156265259\n",
      "current at batch: 110 tensor(5.8785, dtype=torch.float64)\n",
      "9.153080463409424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 111 tensor(5.8091, dtype=torch.float64)\n",
      "8.911471128463745\n",
      "current at batch: 112 tensor(5.8955, dtype=torch.float64)\n",
      "8.897919654846191\n",
      "current at batch: 113 tensor(5.6897, dtype=torch.float64)\n",
      "9.062532424926758\n",
      "current at batch: 114 tensor(5.6905, dtype=torch.float64)\n",
      "9.181167840957642\n",
      "current at batch: 115 tensor(5.5997, dtype=torch.float64)\n",
      "8.736269235610962\n",
      "current at batch: 116 tensor(5.2862, dtype=torch.float64)\n",
      "9.247216939926147\n",
      "current at batch: 117 tensor(5.8535, dtype=torch.float64)\n",
      "8.926484107971191\n",
      "current at batch: 118 tensor(5.5310, dtype=torch.float64)\n",
      "8.690311431884766\n",
      "current at batch: 119 tensor(6.1800, dtype=torch.float64)\n",
      "9.045491218566895\n",
      "current at batch: 120 tensor(5.9724, dtype=torch.float64)\n",
      "9.221117973327637\n",
      "current at batch: 121 tensor(5.5767, dtype=torch.float64)\n",
      "8.823349475860596\n",
      "current at batch: 122 tensor(5.6472, dtype=torch.float64)\n",
      "8.877190589904785\n",
      "current at batch: 123 tensor(5.4202, dtype=torch.float64)\n",
      "9.424317836761475\n",
      "current at batch: 124 tensor(5.8834, dtype=torch.float64)\n",
      "8.598718881607056\n",
      "current at batch: 125 tensor(5.7241, dtype=torch.float64)\n",
      "9.085599184036255\n",
      "current at batch: 126 tensor(5.5218, dtype=torch.float64)\n",
      "8.58916449546814\n",
      "current at batch: 127 tensor(5.8326, dtype=torch.float64)\n",
      "8.99349594116211\n",
      "current at batch: 128 tensor(5.6848, dtype=torch.float64)\n",
      "8.712260484695435\n",
      "current at batch: 129 tensor(5.6568, dtype=torch.float64)\n",
      "9.219199419021606\n",
      "current at batch: 130 tensor(5.8385, dtype=torch.float64)\n",
      "8.84139633178711\n",
      "current at batch: 131 tensor(5.9126, dtype=torch.float64)\n",
      "8.574008226394653\n",
      "current at batch: 132 tensor(5.6470, dtype=torch.float64)\n",
      "8.538235902786255\n",
      "current at batch: 133 tensor(5.7607, dtype=torch.float64)\n",
      "8.53581690788269\n",
      "current at batch: 134 tensor(6.0344, dtype=torch.float64)\n",
      "9.669938802719116\n",
      "current at batch: 135 tensor(5.7418, dtype=torch.float64)\n",
      "8.689280271530151\n",
      "current at batch: 136 tensor(5.8901, dtype=torch.float64)\n",
      "8.910895109176636\n",
      "current at batch: 137 tensor(5.7115, dtype=torch.float64)\n",
      "8.622187376022339\n",
      "current at batch: 138 tensor(5.8303, dtype=torch.float64)\n",
      "9.032639265060425\n",
      "current at batch: 139 tensor(5.4201, dtype=torch.float64)\n",
      "9.11454176902771\n",
      "current at batch: 140 tensor(5.7541, dtype=torch.float64)\n",
      "8.737271070480347\n",
      "current at batch: 141 tensor(5.8617, dtype=torch.float64)\n",
      "9.01801061630249\n",
      "current at batch: 142 tensor(5.7884, dtype=torch.float64)\n",
      "8.858856439590454\n",
      "current at batch: 143 tensor(5.6923, dtype=torch.float64)\n",
      "9.138112783432007\n",
      "current at batch: 144 tensor(5.5241, dtype=torch.float64)\n",
      "9.05502438545227\n",
      "current at batch: 145 tensor(5.8013, dtype=torch.float64)\n",
      "8.368900537490845\n",
      "current at batch: 146 tensor(5.3510, dtype=torch.float64)\n",
      "8.613149881362915\n",
      "current at batch: 147 tensor(5.6556, dtype=torch.float64)\n",
      "8.524278163909912\n",
      "current at batch: 148 tensor(5.6280, dtype=torch.float64)\n",
      "8.62769079208374\n",
      "current at batch: 149 tensor(5.8597, dtype=torch.float64)\n",
      "8.995707273483276\n",
      "current at batch: 150 tensor(5.5444, dtype=torch.float64)\n",
      "8.884377241134644\n",
      "current at batch: 151 tensor(5.7418, dtype=torch.float64)\n",
      "9.157658338546753\n",
      "current at batch: 152 tensor(5.6741, dtype=torch.float64)\n",
      "8.995517253875732\n",
      "current at batch: 153 tensor(5.8425, dtype=torch.float64)\n",
      "8.898439407348633\n",
      "current at batch: 154 tensor(5.6343, dtype=torch.float64)\n",
      "8.666348457336426\n",
      "current at batch: 155 tensor(5.5830, dtype=torch.float64)\n",
      "9.220842123031616\n",
      "current at batch: 156 tensor(5.6095, dtype=torch.float64)\n",
      "8.756255388259888\n",
      "current at batch: 157 tensor(5.7032, dtype=torch.float64)\n",
      "8.889380693435669\n",
      "current at batch: 158 tensor(5.6011, dtype=torch.float64)\n",
      "9.121418714523315\n",
      "current at batch: 159 tensor(5.7844, dtype=torch.float64)\n",
      "8.633333683013916\n",
      "current at batch: 160 tensor(5.3514, dtype=torch.float64)\n",
      "9.058146953582764\n",
      "current at batch: 161 tensor(5.9352, dtype=torch.float64)\n",
      "9.0900239944458\n",
      "current at batch: 162 tensor(5.6887, dtype=torch.float64)\n",
      "8.596669435501099\n",
      "current at batch: 163 tensor(5.9942, dtype=torch.float64)\n",
      "8.989951848983765\n",
      "current at batch: 164 tensor(5.7165, dtype=torch.float64)\n",
      "9.00800371170044\n",
      "current at batch: 165 tensor(5.7058, dtype=torch.float64)\n",
      "8.754326343536377\n",
      "current at batch: 166 tensor(5.7715, dtype=torch.float64)\n",
      "9.182114601135254\n",
      "current at batch: 167 tensor(5.9682, dtype=torch.float64)\n",
      "9.135119915008545\n",
      "current at batch: 168 tensor(5.8053, dtype=torch.float64)\n",
      "8.79335618019104\n",
      "current at batch: 169 tensor(5.4800, dtype=torch.float64)\n",
      "8.554144620895386\n",
      "current at batch: 170 tensor(5.6923, dtype=torch.float64)\n",
      "9.3600332736969\n",
      "current at batch: 171 tensor(5.7775, dtype=torch.float64)\n",
      "9.488434314727783\n",
      "current at batch: 172 tensor(5.6925, dtype=torch.float64)\n",
      "8.63737678527832\n",
      "current at batch: 173 tensor(5.7837, dtype=torch.float64)\n",
      "8.894440412521362\n",
      "current at batch: 174 tensor(5.9829, dtype=torch.float64)\n",
      "8.676852464675903\n",
      "current at batch: 175 tensor(6.1855, dtype=torch.float64)\n",
      "9.218964099884033\n",
      "current at batch: 176 tensor(5.4975, dtype=torch.float64)\n",
      "8.674796104431152\n",
      "current at batch: 177 tensor(5.9394, dtype=torch.float64)\n",
      "8.58171558380127\n",
      "current at batch: 178 tensor(5.5396, dtype=torch.float64)\n",
      "8.477764368057251\n",
      "current at batch: 179 tensor(6.0145, dtype=torch.float64)\n",
      "8.778358697891235\n",
      "current at batch: 180 tensor(5.3501, dtype=torch.float64)\n",
      "9.317770719528198\n",
      "current at batch: 181 tensor(5.5354, dtype=torch.float64)\n",
      "9.094583988189697\n",
      "current at batch: 182 tensor(5.7128, dtype=torch.float64)\n",
      "9.357294082641602\n",
      "current at batch: 183 tensor(5.7103, dtype=torch.float64)\n",
      "8.75433874130249\n",
      "current at batch: 184 tensor(6.0506, dtype=torch.float64)\n",
      "9.033517599105835\n",
      "current at batch: 185 tensor(5.7337, dtype=torch.float64)\n",
      "8.928464889526367\n",
      "current at batch: 186 tensor(5.8532, dtype=torch.float64)\n",
      "8.818385362625122\n",
      "current at batch: 187 tensor(5.7221, dtype=torch.float64)\n",
      "9.14737057685852\n",
      "current at batch: 188 tensor(5.8319, dtype=torch.float64)\n",
      "8.873965978622437\n",
      "current at batch: 189 tensor(5.0501, dtype=torch.float64)\n",
      "4.13649582862854\n",
      "epoch =  4 \n",
      " tensor(1087.6160, dtype=torch.float64)\n",
      "current at batch: 1 tensor(5.3116, dtype=torch.float64)\n",
      "8.392576694488525\n",
      "current at batch: 2 tensor(6.0433, dtype=torch.float64)\n",
      "8.945476531982422\n",
      "current at batch: 3 tensor(5.5711, dtype=torch.float64)\n",
      "8.62731409072876\n",
      "current at batch: 4 tensor(5.5523, dtype=torch.float64)\n",
      "8.115928649902344\n",
      "current at batch: 5 tensor(5.6797, dtype=torch.float64)\n",
      "8.813880205154419\n",
      "current at batch: 6 tensor(5.9689, dtype=torch.float64)\n",
      "8.789664506912231\n",
      "current at batch: 7 tensor(5.9814, dtype=torch.float64)\n",
      "8.539568901062012\n",
      "current at batch: 8 tensor(5.6179, dtype=torch.float64)\n",
      "8.522704839706421\n",
      "current at batch: 9 tensor(5.5664, dtype=torch.float64)\n",
      "8.43660855293274\n",
      "current at batch: 10 tensor(5.6749, dtype=torch.float64)\n",
      "8.837995767593384\n",
      "current at batch: 11 tensor(5.5394, dtype=torch.float64)\n",
      "8.499178409576416\n",
      "current at batch: 12 tensor(5.3341, dtype=torch.float64)\n",
      "8.318713188171387\n",
      "current at batch: 13 tensor(5.4092, dtype=torch.float64)\n",
      "9.192091941833496\n",
      "current at batch: 14 tensor(5.2503, dtype=torch.float64)\n",
      "8.777556419372559\n",
      "current at batch: 15 tensor(5.4416, dtype=torch.float64)\n",
      "8.353607892990112\n",
      "current at batch: 16 tensor(5.6789, dtype=torch.float64)\n",
      "8.822503089904785\n",
      "current at batch: 17 tensor(5.4046, dtype=torch.float64)\n",
      "8.377509593963623\n",
      "current at batch: 18 tensor(5.6554, dtype=torch.float64)\n",
      "8.888872146606445\n",
      "current at batch: 19 tensor(5.2452, dtype=torch.float64)\n",
      "8.27896237373352\n",
      "current at batch: 20 tensor(5.2009, dtype=torch.float64)\n",
      "8.57489800453186\n",
      "current at batch: 21 tensor(5.6044, dtype=torch.float64)\n",
      "8.579124927520752\n",
      "current at batch: 22 tensor(5.3122, dtype=torch.float64)\n",
      "8.360487699508667\n",
      "current at batch: 23 tensor(5.7101, dtype=torch.float64)\n",
      "8.414984464645386\n",
      "current at batch: 24 tensor(5.3999, dtype=torch.float64)\n",
      "8.654762744903564\n",
      "current at batch: 25 tensor(5.6679, dtype=torch.float64)\n",
      "7.810142517089844\n",
      "current at batch: 26 tensor(5.5116, dtype=torch.float64)\n",
      "8.43700098991394\n",
      "current at batch: 27 tensor(5.5307, dtype=torch.float64)\n",
      "8.434013843536377\n",
      "current at batch: 28 tensor(5.4255, dtype=torch.float64)\n",
      "8.901830434799194\n",
      "current at batch: 29 tensor(5.2977, dtype=torch.float64)\n",
      "8.545108079910278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 30 tensor(5.2329, dtype=torch.float64)\n",
      "8.769766092300415\n",
      "current at batch: 31 tensor(5.3220, dtype=torch.float64)\n",
      "8.394084692001343\n",
      "current at batch: 32 tensor(5.5283, dtype=torch.float64)\n",
      "8.969879150390625\n",
      "current at batch: 33 tensor(5.6855, dtype=torch.float64)\n",
      "8.31155276298523\n",
      "current at batch: 34 tensor(5.5424, dtype=torch.float64)\n",
      "8.956467628479004\n",
      "current at batch: 35 tensor(5.4610, dtype=torch.float64)\n",
      "8.270448923110962\n",
      "current at batch: 36 tensor(5.7513, dtype=torch.float64)\n",
      "8.902381420135498\n",
      "current at batch: 37 tensor(5.5939, dtype=torch.float64)\n",
      "8.947387456893921\n",
      "current at batch: 38 tensor(5.5719, dtype=torch.float64)\n",
      "8.787295818328857\n",
      "current at batch: 39 tensor(5.5418, dtype=torch.float64)\n",
      "8.294414281845093\n",
      "current at batch: 40 tensor(5.4823, dtype=torch.float64)\n",
      "8.45104432106018\n",
      "current at batch: 41 tensor(5.6606, dtype=torch.float64)\n",
      "8.439854145050049\n",
      "current at batch: 42 tensor(5.5256, dtype=torch.float64)\n",
      "8.81426477432251\n",
      "current at batch: 43 tensor(5.7067, dtype=torch.float64)\n",
      "8.409480810165405\n",
      "current at batch: 44 tensor(5.2837, dtype=torch.float64)\n",
      "8.401613235473633\n",
      "current at batch: 45 tensor(5.7030, dtype=torch.float64)\n",
      "8.437596559524536\n",
      "current at batch: 46 tensor(5.4582, dtype=torch.float64)\n",
      "8.045434474945068\n",
      "current at batch: 47 tensor(5.9415, dtype=torch.float64)\n",
      "9.083445310592651\n",
      "current at batch: 48 tensor(5.7623, dtype=torch.float64)\n",
      "8.616638422012329\n",
      "current at batch: 49 tensor(5.5149, dtype=torch.float64)\n",
      "8.443057298660278\n",
      "current at batch: 50 tensor(5.6177, dtype=torch.float64)\n",
      "9.352761268615723\n",
      "current at batch: 51 tensor(5.5383, dtype=torch.float64)\n",
      "8.771793127059937\n",
      "current at batch: 52 tensor(5.6990, dtype=torch.float64)\n",
      "8.87431025505066\n",
      "current at batch: 53 tensor(5.4830, dtype=torch.float64)\n",
      "8.578982591629028\n",
      "current at batch: 54 tensor(5.4235, dtype=torch.float64)\n",
      "8.459206342697144\n",
      "current at batch: 55 tensor(5.7139, dtype=torch.float64)\n",
      "8.828335523605347\n",
      "current at batch: 56 tensor(5.7564, dtype=torch.float64)\n",
      "8.545578002929688\n",
      "current at batch: 57 tensor(6.1375, dtype=torch.float64)\n",
      "9.107673406600952\n",
      "current at batch: 58 tensor(5.3506, dtype=torch.float64)\n",
      "8.306945323944092\n",
      "current at batch: 59 tensor(5.2210, dtype=torch.float64)\n",
      "8.384101629257202\n",
      "current at batch: 60 tensor(5.3205, dtype=torch.float64)\n",
      "8.591110229492188\n",
      "current at batch: 61 tensor(5.9207, dtype=torch.float64)\n",
      "8.342139720916748\n",
      "current at batch: 62 tensor(5.6498, dtype=torch.float64)\n",
      "8.95086669921875\n",
      "current at batch: 63 tensor(5.7328, dtype=torch.float64)\n",
      "8.8790442943573\n",
      "current at batch: 64 tensor(5.4995, dtype=torch.float64)\n",
      "8.573124885559082\n",
      "current at batch: 65 tensor(5.6734, dtype=torch.float64)\n",
      "8.764961242675781\n",
      "current at batch: 66 tensor(5.6875, dtype=torch.float64)\n",
      "8.522265434265137\n",
      "current at batch: 67 tensor(5.2586, dtype=torch.float64)\n",
      "8.878946542739868\n",
      "current at batch: 68 tensor(5.6774, dtype=torch.float64)\n",
      "8.575099468231201\n",
      "current at batch: 69 tensor(5.5964, dtype=torch.float64)\n",
      "8.330425500869751\n",
      "current at batch: 70 tensor(5.6633, dtype=torch.float64)\n",
      "8.873860597610474\n",
      "current at batch: 71 tensor(5.5023, dtype=torch.float64)\n",
      "8.456515073776245\n",
      "current at batch: 72 tensor(5.2881, dtype=torch.float64)\n",
      "8.500593662261963\n",
      "current at batch: 73 tensor(5.5731, dtype=torch.float64)\n",
      "8.542576551437378\n",
      "current at batch: 74 tensor(5.4228, dtype=torch.float64)\n",
      "8.79367184638977\n",
      "current at batch: 75 tensor(5.2711, dtype=torch.float64)\n",
      "8.541320323944092\n",
      "current at batch: 76 tensor(5.3575, dtype=torch.float64)\n",
      "8.557724714279175\n",
      "current at batch: 77 tensor(5.1915, dtype=torch.float64)\n",
      "8.629682064056396\n",
      "current at batch: 78 tensor(5.3202, dtype=torch.float64)\n",
      "9.087556600570679\n",
      "current at batch: 79 tensor(5.0987, dtype=torch.float64)\n",
      "8.181463956832886\n",
      "current at batch: 80 tensor(5.5302, dtype=torch.float64)\n",
      "8.498378276824951\n",
      "current at batch: 81 tensor(5.1973, dtype=torch.float64)\n",
      "8.335941791534424\n",
      "current at batch: 82 tensor(5.5631, dtype=torch.float64)\n",
      "8.437669515609741\n",
      "current at batch: 83 tensor(5.7016, dtype=torch.float64)\n",
      "8.715217590332031\n",
      "current at batch: 84 tensor(5.7172, dtype=torch.float64)\n",
      "8.835283994674683\n",
      "current at batch: 85 tensor(5.5978, dtype=torch.float64)\n",
      "8.843936204910278\n",
      "current at batch: 86 tensor(5.3409, dtype=torch.float64)\n",
      "8.545127630233765\n",
      "current at batch: 87 tensor(5.7218, dtype=torch.float64)\n",
      "8.539656400680542\n",
      "current at batch: 88 tensor(5.4084, dtype=torch.float64)\n",
      "8.357975959777832\n",
      "current at batch: 89 tensor(5.5572, dtype=torch.float64)\n",
      "8.44226598739624\n",
      "current at batch: 90 tensor(5.6456, dtype=torch.float64)\n",
      "8.3835928440094\n",
      "current at batch: 91 tensor(5.4930, dtype=torch.float64)\n",
      "8.173813104629517\n",
      "current at batch: 92 tensor(5.4250, dtype=torch.float64)\n",
      "8.519107580184937\n",
      "current at batch: 93 tensor(5.8373, dtype=torch.float64)\n",
      "8.526590347290039\n",
      "current at batch: 94 tensor(5.4313, dtype=torch.float64)\n",
      "8.172837495803833\n",
      "current at batch: 95 tensor(5.4940, dtype=torch.float64)\n",
      "8.939859628677368\n",
      "current at batch: 96 tensor(5.2130, dtype=torch.float64)\n",
      "7.960702419281006\n",
      "current at batch: 97 tensor(5.8836, dtype=torch.float64)\n",
      "8.798364400863647\n",
      "current at batch: 98 tensor(5.5278, dtype=torch.float64)\n",
      "8.544152975082397\n",
      "current at batch: 99 tensor(5.4519, dtype=torch.float64)\n",
      "8.595815896987915\n",
      "current at batch: 100 tensor(5.4844, dtype=torch.float64)\n",
      "8.233048677444458\n",
      "current at batch: 101 tensor(6.0222, dtype=torch.float64)\n",
      "9.241991996765137\n",
      "current at batch: 102 tensor(5.5215, dtype=torch.float64)\n",
      "8.867533206939697\n",
      "current at batch: 103 tensor(5.8035, dtype=torch.float64)\n",
      "8.204348087310791\n",
      "current at batch: 104 tensor(5.6659, dtype=torch.float64)\n",
      "8.714081048965454\n",
      "current at batch: 105 tensor(5.6048, dtype=torch.float64)\n",
      "8.555166006088257\n",
      "current at batch: 106 tensor(5.8144, dtype=torch.float64)\n",
      "8.467751264572144\n",
      "current at batch: 107 tensor(5.3400, dtype=torch.float64)\n",
      "8.06886076927185\n",
      "current at batch: 108 tensor(5.9019, dtype=torch.float64)\n",
      "8.429749250411987\n",
      "current at batch: 109 tensor(5.3386, dtype=torch.float64)\n",
      "8.220857381820679\n",
      "current at batch: 110 tensor(5.6433, dtype=torch.float64)\n",
      "8.833028078079224\n",
      "current at batch: 111 tensor(5.7586, dtype=torch.float64)\n",
      "8.836840152740479\n",
      "current at batch: 112 tensor(5.4324, dtype=torch.float64)\n",
      "8.28239107131958\n",
      "current at batch: 113 tensor(5.4384, dtype=torch.float64)\n",
      "7.963750123977661\n",
      "current at batch: 114 tensor(5.3757, dtype=torch.float64)\n",
      "8.432497262954712\n",
      "current at batch: 115 tensor(5.5178, dtype=torch.float64)\n",
      "8.535040378570557\n",
      "current at batch: 116 tensor(5.7042, dtype=torch.float64)\n",
      "8.515056610107422\n",
      "current at batch: 117 tensor(5.4368, dtype=torch.float64)\n",
      "8.528063774108887\n",
      "current at batch: 118 tensor(5.4716, dtype=torch.float64)\n",
      "8.02125859260559\n",
      "current at batch: 119 tensor(5.3264, dtype=torch.float64)\n",
      "8.862868070602417\n",
      "current at batch: 120 tensor(5.5811, dtype=torch.float64)\n",
      "8.790785551071167\n",
      "current at batch: 121 tensor(5.3024, dtype=torch.float64)\n",
      "8.229352951049805\n",
      "current at batch: 122 tensor(5.7832, dtype=torch.float64)\n",
      "8.612689733505249\n",
      "current at batch: 123 tensor(5.4250, dtype=torch.float64)\n",
      "8.175709009170532\n",
      "current at batch: 124 tensor(5.5169, dtype=torch.float64)\n",
      "8.31991720199585\n",
      "current at batch: 125 tensor(5.5679, dtype=torch.float64)\n",
      "8.52857780456543\n",
      "current at batch: 126 tensor(5.4277, dtype=torch.float64)\n",
      "8.400048971176147\n",
      "current at batch: 127 tensor(5.7825, dtype=torch.float64)\n",
      "8.683699131011963\n",
      "current at batch: 128 tensor(5.5294, dtype=torch.float64)\n",
      "8.278932571411133\n",
      "current at batch: 129 tensor(5.7231, dtype=torch.float64)\n",
      "8.665770530700684\n",
      "current at batch: 130 tensor(5.6800, dtype=torch.float64)\n",
      "8.726706981658936\n",
      "current at batch: 131 tensor(5.6618, dtype=torch.float64)\n",
      "8.503562450408936\n",
      "current at batch: 132 tensor(5.2900, dtype=torch.float64)\n",
      "8.62466049194336\n",
      "current at batch: 133 tensor(5.6954, dtype=torch.float64)\n",
      "8.727708339691162\n",
      "current at batch: 134 tensor(5.2599, dtype=torch.float64)\n",
      "8.537071704864502\n",
      "current at batch: 135 tensor(5.5541, dtype=torch.float64)\n",
      "7.98969030380249\n",
      "current at batch: 136 tensor(5.7229, dtype=torch.float64)\n",
      "8.588128089904785\n",
      "current at batch: 137 tensor(5.3417, dtype=torch.float64)\n",
      "8.160321712493896\n",
      "current at batch: 138 tensor(5.4502, dtype=torch.float64)\n",
      "8.081046104431152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 139 tensor(5.3691, dtype=torch.float64)\n",
      "8.49067211151123\n",
      "current at batch: 140 tensor(5.5980, dtype=torch.float64)\n",
      "8.566092252731323\n",
      "current at batch: 141 tensor(5.5512, dtype=torch.float64)\n",
      "8.470619678497314\n",
      "current at batch: 142 tensor(5.5804, dtype=torch.float64)\n",
      "8.435226678848267\n",
      "current at batch: 143 tensor(5.5842, dtype=torch.float64)\n",
      "8.627290964126587\n",
      "current at batch: 144 tensor(5.6794, dtype=torch.float64)\n",
      "8.677538394927979\n",
      "current at batch: 145 tensor(5.5529, dtype=torch.float64)\n",
      "7.993195533752441\n",
      "current at batch: 146 tensor(5.6999, dtype=torch.float64)\n",
      "8.597615480422974\n",
      "current at batch: 147 tensor(5.4557, dtype=torch.float64)\n",
      "8.448508262634277\n",
      "current at batch: 148 tensor(5.5545, dtype=torch.float64)\n",
      "8.309453248977661\n",
      "current at batch: 149 tensor(5.4645, dtype=torch.float64)\n",
      "8.662712574005127\n",
      "current at batch: 150 tensor(5.4812, dtype=torch.float64)\n",
      "8.420563459396362\n",
      "current at batch: 151 tensor(5.4604, dtype=torch.float64)\n",
      "8.284017562866211\n",
      "current at batch: 152 tensor(5.5697, dtype=torch.float64)\n",
      "8.34406590461731\n",
      "current at batch: 153 tensor(5.4049, dtype=torch.float64)\n",
      "8.238412618637085\n",
      "current at batch: 154 tensor(5.4976, dtype=torch.float64)\n",
      "8.053305625915527\n",
      "current at batch: 155 tensor(5.5998, dtype=torch.float64)\n",
      "8.800223112106323\n",
      "current at batch: 156 tensor(5.4553, dtype=torch.float64)\n",
      "8.535626411437988\n",
      "current at batch: 157 tensor(5.5151, dtype=torch.float64)\n",
      "8.30333423614502\n",
      "current at batch: 158 tensor(5.5824, dtype=torch.float64)\n",
      "8.181873559951782\n",
      "current at batch: 159 tensor(5.4839, dtype=torch.float64)\n",
      "8.439684391021729\n",
      "current at batch: 160 tensor(5.4680, dtype=torch.float64)\n",
      "8.503166198730469\n",
      "current at batch: 161 tensor(5.3882, dtype=torch.float64)\n",
      "8.226444721221924\n",
      "current at batch: 162 tensor(5.7629, dtype=torch.float64)\n",
      "8.846357583999634\n",
      "current at batch: 163 tensor(5.6222, dtype=torch.float64)\n",
      "8.158354759216309\n",
      "current at batch: 164 tensor(5.6280, dtype=torch.float64)\n",
      "8.191379308700562\n",
      "current at batch: 165 tensor(5.8752, dtype=torch.float64)\n",
      "8.148958921432495\n",
      "current at batch: 166 tensor(5.8925, dtype=torch.float64)\n",
      "8.673726081848145\n",
      "current at batch: 167 tensor(5.6766, dtype=torch.float64)\n",
      "8.644704580307007\n",
      "current at batch: 168 tensor(5.3994, dtype=torch.float64)\n",
      "8.817620038986206\n",
      "current at batch: 169 tensor(5.2890, dtype=torch.float64)\n",
      "8.1135094165802\n",
      "current at batch: 170 tensor(5.8233, dtype=torch.float64)\n",
      "8.681439399719238\n",
      "current at batch: 171 tensor(5.5748, dtype=torch.float64)\n",
      "8.891880750656128\n",
      "current at batch: 172 tensor(5.9321, dtype=torch.float64)\n",
      "8.10492467880249\n",
      "current at batch: 173 tensor(5.3793, dtype=torch.float64)\n",
      "8.463074207305908\n",
      "current at batch: 174 tensor(5.1959, dtype=torch.float64)\n",
      "8.487629652023315\n",
      "current at batch: 175 tensor(5.5876, dtype=torch.float64)\n",
      "8.059283971786499\n",
      "current at batch: 176 tensor(5.3994, dtype=torch.float64)\n",
      "8.320486307144165\n",
      "current at batch: 177 tensor(5.6372, dtype=torch.float64)\n",
      "8.535653829574585\n",
      "current at batch: 178 tensor(5.7225, dtype=torch.float64)\n",
      "8.65285611152649\n",
      "current at batch: 179 tensor(5.6604, dtype=torch.float64)\n",
      "8.659873247146606\n",
      "current at batch: 180 tensor(5.6054, dtype=torch.float64)\n",
      "7.915786504745483\n",
      "current at batch: 181 tensor(5.4421, dtype=torch.float64)\n",
      "8.60317325592041\n",
      "current at batch: 182 tensor(5.2317, dtype=torch.float64)\n",
      "8.071792840957642\n",
      "current at batch: 183 tensor(5.9321, dtype=torch.float64)\n",
      "7.850133657455444\n",
      "current at batch: 184 tensor(5.7512, dtype=torch.float64)\n",
      "8.821646928787231\n",
      "current at batch: 185 tensor(5.4833, dtype=torch.float64)\n",
      "8.112821578979492\n",
      "current at batch: 186 tensor(5.4901, dtype=torch.float64)\n",
      "8.390521764755249\n",
      "current at batch: 187 tensor(5.5267, dtype=torch.float64)\n",
      "8.489091396331787\n",
      "current at batch: 188 tensor(5.6515, dtype=torch.float64)\n",
      "8.424547672271729\n",
      "current at batch: 189 tensor(5.7573, dtype=torch.float64)\n",
      "3.9833590984344482\n",
      "epoch =  5 \n",
      " tensor(1049.0641, dtype=torch.float64)\n",
      "current at batch: 1 tensor(5.5209, dtype=torch.float64)\n",
      "8.046644687652588\n",
      "current at batch: 2 tensor(5.5833, dtype=torch.float64)\n",
      "8.23342514038086\n",
      "current at batch: 3 tensor(5.4586, dtype=torch.float64)\n",
      "8.070291757583618\n",
      "current at batch: 4 tensor(5.4464, dtype=torch.float64)\n",
      "8.55705714225769\n",
      "current at batch: 5 tensor(5.4705, dtype=torch.float64)\n",
      "8.351007461547852\n",
      "current at batch: 6 tensor(5.4513, dtype=torch.float64)\n",
      "7.964716196060181\n",
      "current at batch: 7 tensor(5.5831, dtype=torch.float64)\n",
      "8.519638061523438\n",
      "current at batch: 8 tensor(5.2810, dtype=torch.float64)\n",
      "7.960712671279907\n",
      "current at batch: 9 tensor(5.7432, dtype=torch.float64)\n",
      "8.071863889694214\n",
      "current at batch: 10 tensor(5.1577, dtype=torch.float64)\n",
      "8.255536079406738\n",
      "current at batch: 11 tensor(5.5566, dtype=torch.float64)\n",
      "7.871648788452148\n",
      "current at batch: 12 tensor(5.6006, dtype=torch.float64)\n",
      "8.098856210708618\n",
      "current at batch: 13 tensor(5.6668, dtype=torch.float64)\n",
      "8.291489601135254\n",
      "current at batch: 14 tensor(5.2630, dtype=torch.float64)\n",
      "8.117833614349365\n",
      "current at batch: 15 tensor(5.2837, dtype=torch.float64)\n",
      "8.097321033477783\n",
      "current at batch: 16 tensor(5.6443, dtype=torch.float64)\n",
      "8.123330116271973\n",
      "current at batch: 17 tensor(5.3024, dtype=torch.float64)\n",
      "8.072850942611694\n",
      "current at batch: 18 tensor(5.1378, dtype=torch.float64)\n",
      "8.248417377471924\n",
      "current at batch: 19 tensor(5.4382, dtype=torch.float64)\n",
      "8.089305400848389\n",
      "current at batch: 20 tensor(5.4573, dtype=torch.float64)\n",
      "7.856668710708618\n",
      "current at batch: 21 tensor(5.3352, dtype=torch.float64)\n",
      "8.009768009185791\n",
      "current at batch: 22 tensor(5.1310, dtype=torch.float64)\n",
      "7.879215478897095\n",
      "current at batch: 23 tensor(5.4716, dtype=torch.float64)\n",
      "7.873176097869873\n",
      "current at batch: 24 tensor(5.2073, dtype=torch.float64)\n",
      "7.86186146736145\n",
      "current at batch: 25 tensor(5.1951, dtype=torch.float64)\n",
      "7.867314577102661\n",
      "current at batch: 26 tensor(5.3065, dtype=torch.float64)\n",
      "7.797397136688232\n",
      "current at batch: 27 tensor(5.6096, dtype=torch.float64)\n",
      "8.116016387939453\n",
      "current at batch: 28 tensor(5.2317, dtype=torch.float64)\n",
      "7.901170492172241\n",
      "current at batch: 29 tensor(5.4885, dtype=torch.float64)\n",
      "8.026856899261475\n",
      "current at batch: 30 tensor(5.7508, dtype=torch.float64)\n",
      "7.65185284614563\n",
      "current at batch: 31 tensor(5.5266, dtype=torch.float64)\n",
      "8.375543117523193\n",
      "current at batch: 32 tensor(5.6276, dtype=torch.float64)\n",
      "7.964284658432007\n",
      "current at batch: 33 tensor(5.5018, dtype=torch.float64)\n",
      "8.120369672775269\n",
      "current at batch: 34 tensor(5.3122, dtype=torch.float64)\n",
      "8.064343214035034\n",
      "current at batch: 35 tensor(5.0877, dtype=torch.float64)\n",
      "8.26743197441101\n",
      "current at batch: 36 tensor(5.6185, dtype=torch.float64)\n",
      "7.829134702682495\n",
      "current at batch: 37 tensor(5.4155, dtype=torch.float64)\n",
      "8.44155764579773\n",
      "current at batch: 38 tensor(5.2425, dtype=torch.float64)\n",
      "8.174366474151611\n",
      "current at batch: 39 tensor(5.3635, dtype=torch.float64)\n",
      "7.820235013961792\n",
      "current at batch: 40 tensor(5.1896, dtype=torch.float64)\n",
      "8.03926944732666\n",
      "current at batch: 41 tensor(5.3536, dtype=torch.float64)\n",
      "7.67151665687561\n",
      "current at batch: 42 tensor(5.4532, dtype=torch.float64)\n",
      "8.555448293685913\n",
      "current at batch: 43 tensor(5.1347, dtype=torch.float64)\n",
      "7.868647336959839\n",
      "current at batch: 44 tensor(5.5559, dtype=torch.float64)\n",
      "8.326488733291626\n",
      "current at batch: 45 tensor(5.5152, dtype=torch.float64)\n",
      "8.294466257095337\n",
      "current at batch: 46 tensor(5.3632, dtype=torch.float64)\n",
      "8.101912498474121\n",
      "current at batch: 47 tensor(5.9672, dtype=torch.float64)\n",
      "7.759583950042725\n",
      "current at batch: 48 tensor(5.7761, dtype=torch.float64)\n",
      "8.394524097442627\n",
      "current at batch: 49 tensor(5.6764, dtype=torch.float64)\n",
      "8.134907722473145\n",
      "current at batch: 50 tensor(5.3697, dtype=torch.float64)\n",
      "8.079854249954224\n",
      "current at batch: 51 tensor(5.3517, dtype=torch.float64)\n",
      "7.891295671463013\n",
      "current at batch: 52 tensor(5.1098, dtype=torch.float64)\n",
      "8.237426519393921\n",
      "current at batch: 53 tensor(5.4842, dtype=torch.float64)\n",
      "8.389601707458496\n",
      "current at batch: 54 tensor(5.3347, dtype=torch.float64)\n",
      "7.820612192153931\n",
      "current at batch: 55 tensor(5.5220, dtype=torch.float64)\n",
      "8.335530519485474\n",
      "current at batch: 56 tensor(5.4974, dtype=torch.float64)\n",
      "7.908255577087402\n",
      "current at batch: 57 tensor(5.6233, dtype=torch.float64)\n",
      "8.03200387954712\n",
      "current at batch: 58 tensor(5.4605, dtype=torch.float64)\n",
      "8.603687047958374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 59 tensor(5.2843, dtype=torch.float64)\n",
      "8.121371746063232\n",
      "current at batch: 60 tensor(5.5693, dtype=torch.float64)\n",
      "8.441057920455933\n",
      "current at batch: 61 tensor(5.2304, dtype=torch.float64)\n",
      "8.063348770141602\n",
      "current at batch: 62 tensor(5.1526, dtype=torch.float64)\n",
      "8.412564039230347\n",
      "current at batch: 63 tensor(5.4659, dtype=torch.float64)\n",
      "7.923699378967285\n",
      "current at batch: 64 tensor(5.5737, dtype=torch.float64)\n",
      "8.672750473022461\n",
      "current at batch: 65 tensor(5.6871, dtype=torch.float64)\n",
      "8.085856199264526\n",
      "current at batch: 66 tensor(5.3130, dtype=torch.float64)\n",
      "8.011257886886597\n",
      "current at batch: 67 tensor(5.3983, dtype=torch.float64)\n",
      "8.44034719467163\n",
      "current at batch: 68 tensor(5.2449, dtype=torch.float64)\n",
      "8.287593364715576\n",
      "current at batch: 69 tensor(5.3491, dtype=torch.float64)\n",
      "7.974358797073364\n",
      "current at batch: 70 tensor(5.5759, dtype=torch.float64)\n",
      "7.92225980758667\n",
      "current at batch: 71 tensor(5.5606, dtype=torch.float64)\n",
      "8.162440538406372\n",
      "current at batch: 72 tensor(5.3987, dtype=torch.float64)\n",
      "7.833734035491943\n",
      "current at batch: 73 tensor(5.7099, dtype=torch.float64)\n",
      "8.27649188041687\n",
      "current at batch: 74 tensor(5.3815, dtype=torch.float64)\n",
      "8.495710611343384\n",
      "current at batch: 75 tensor(5.5682, dtype=torch.float64)\n",
      "8.01438283920288\n",
      "current at batch: 76 tensor(5.4692, dtype=torch.float64)\n",
      "7.733599901199341\n",
      "current at batch: 77 tensor(5.6653, dtype=torch.float64)\n",
      "8.362054824829102\n",
      "current at batch: 78 tensor(5.5597, dtype=torch.float64)\n",
      "7.86932897567749\n",
      "current at batch: 79 tensor(5.4605, dtype=torch.float64)\n",
      "7.882820129394531\n",
      "current at batch: 80 tensor(5.3577, dtype=torch.float64)\n",
      "8.15851879119873\n",
      "current at batch: 81 tensor(5.2531, dtype=torch.float64)\n",
      "7.687566757202148\n",
      "current at batch: 82 tensor(5.5884, dtype=torch.float64)\n",
      "8.197324752807617\n",
      "current at batch: 83 tensor(5.3079, dtype=torch.float64)\n",
      "8.041332960128784\n",
      "current at batch: 84 tensor(5.6507, dtype=torch.float64)\n",
      "7.901253938674927\n",
      "current at batch: 85 tensor(5.4122, dtype=torch.float64)\n",
      "8.249995708465576\n",
      "current at batch: 86 tensor(5.2893, dtype=torch.float64)\n",
      "8.057378768920898\n",
      "current at batch: 87 tensor(5.5967, dtype=torch.float64)\n",
      "7.997100830078125\n",
      "current at batch: 88 tensor(5.5010, dtype=torch.float64)\n",
      "8.554247617721558\n",
      "current at batch: 89 tensor(5.1906, dtype=torch.float64)\n",
      "7.675581216812134\n",
      "current at batch: 90 tensor(5.3764, dtype=torch.float64)\n",
      "7.829169034957886\n",
      "current at batch: 91 tensor(5.5934, dtype=torch.float64)\n",
      "8.136892080307007\n",
      "current at batch: 92 tensor(5.2715, dtype=torch.float64)\n",
      "7.677583932876587\n",
      "current at batch: 93 tensor(5.7001, dtype=torch.float64)\n",
      "7.703089714050293\n",
      "current at batch: 94 tensor(5.4832, dtype=torch.float64)\n",
      "8.21249532699585\n",
      "current at batch: 95 tensor(5.6513, dtype=torch.float64)\n",
      "8.172495126724243\n",
      "current at batch: 96 tensor(5.1262, dtype=torch.float64)\n",
      "8.033736228942871\n",
      "current at batch: 97 tensor(5.6974, dtype=torch.float64)\n",
      "7.588494300842285\n",
      "current at batch: 98 tensor(5.8662, dtype=torch.float64)\n",
      "8.068394660949707\n",
      "current at batch: 99 tensor(5.1798, dtype=torch.float64)\n",
      "7.899718523025513\n",
      "current at batch: 100 tensor(5.6642, dtype=torch.float64)\n",
      "8.065838813781738\n",
      "current at batch: 101 tensor(5.4308, dtype=torch.float64)\n",
      "7.98528265953064\n",
      "current at batch: 102 tensor(5.4838, dtype=torch.float64)\n",
      "8.174441814422607\n",
      "current at batch: 103 tensor(5.3580, dtype=torch.float64)\n",
      "7.794626474380493\n",
      "current at batch: 104 tensor(5.1179, dtype=torch.float64)\n",
      "8.141325235366821\n",
      "current at batch: 105 tensor(5.4102, dtype=torch.float64)\n",
      "8.02087688446045\n",
      "current at batch: 106 tensor(5.3449, dtype=torch.float64)\n",
      "8.26397180557251\n",
      "current at batch: 107 tensor(5.7239, dtype=torch.float64)\n",
      "7.996788263320923\n",
      "current at batch: 108 tensor(5.2016, dtype=torch.float64)\n",
      "8.328017950057983\n",
      "current at batch: 109 tensor(5.2580, dtype=torch.float64)\n",
      "7.551939487457275\n",
      "current at batch: 110 tensor(5.6296, dtype=torch.float64)\n",
      "8.241414070129395\n",
      "current at batch: 111 tensor(5.2488, dtype=torch.float64)\n",
      "7.985230445861816\n",
      "current at batch: 112 tensor(5.5584, dtype=torch.float64)\n",
      "8.11632490158081\n",
      "current at batch: 113 tensor(5.2336, dtype=torch.float64)\n",
      "8.142932176589966\n",
      "current at batch: 114 tensor(5.5516, dtype=torch.float64)\n",
      "8.411099910736084\n",
      "current at batch: 115 tensor(5.3917, dtype=torch.float64)\n",
      "7.767599105834961\n",
      "current at batch: 116 tensor(5.4486, dtype=torch.float64)\n",
      "8.091839075088501\n",
      "current at batch: 117 tensor(5.1722, dtype=torch.float64)\n",
      "7.876835823059082\n",
      "current at batch: 118 tensor(5.3941, dtype=torch.float64)\n",
      "8.226468324661255\n",
      "current at batch: 119 tensor(5.6493, dtype=torch.float64)\n",
      "8.189181566238403\n",
      "current at batch: 120 tensor(5.3407, dtype=torch.float64)\n",
      "8.17186450958252\n",
      "current at batch: 121 tensor(5.4857, dtype=torch.float64)\n",
      "8.475259780883789\n",
      "current at batch: 122 tensor(5.7181, dtype=torch.float64)\n",
      "8.168206453323364\n",
      "current at batch: 123 tensor(5.1633, dtype=torch.float64)\n",
      "8.406582355499268\n",
      "current at batch: 124 tensor(5.6776, dtype=torch.float64)\n",
      "8.046274423599243\n",
      "current at batch: 125 tensor(5.5767, dtype=torch.float64)\n",
      "8.415019750595093\n",
      "current at batch: 126 tensor(5.6266, dtype=torch.float64)\n",
      "7.970742464065552\n",
      "current at batch: 127 tensor(5.5168, dtype=torch.float64)\n",
      "7.88527774810791\n",
      "current at batch: 128 tensor(5.3302, dtype=torch.float64)\n",
      "7.9288551807403564\n",
      "current at batch: 129 tensor(5.1648, dtype=torch.float64)\n",
      "8.221413135528564\n",
      "current at batch: 130 tensor(5.3078, dtype=torch.float64)\n",
      "7.778081655502319\n",
      "current at batch: 131 tensor(5.6041, dtype=torch.float64)\n",
      "8.333194255828857\n",
      "current at batch: 132 tensor(5.5311, dtype=torch.float64)\n",
      "8.152364253997803\n",
      "current at batch: 133 tensor(5.1802, dtype=torch.float64)\n",
      "8.230950593948364\n",
      "current at batch: 134 tensor(5.3442, dtype=torch.float64)\n",
      "8.075114965438843\n",
      "current at batch: 135 tensor(5.6101, dtype=torch.float64)\n",
      "7.962346315383911\n",
      "current at batch: 136 tensor(5.4785, dtype=torch.float64)\n",
      "8.103816270828247\n",
      "current at batch: 137 tensor(5.3914, dtype=torch.float64)\n",
      "7.5819411277771\n",
      "current at batch: 138 tensor(5.2341, dtype=torch.float64)\n",
      "8.325974464416504\n",
      "current at batch: 139 tensor(5.4838, dtype=torch.float64)\n",
      "7.82750129699707\n",
      "current at batch: 140 tensor(5.4517, dtype=torch.float64)\n",
      "8.14115285873413\n",
      "current at batch: 141 tensor(5.5634, dtype=torch.float64)\n",
      "7.9818713665008545\n",
      "current at batch: 142 tensor(5.2728, dtype=torch.float64)\n",
      "7.739836931228638\n",
      "current at batch: 143 tensor(5.5675, dtype=torch.float64)\n",
      "8.286000728607178\n",
      "current at batch: 144 tensor(5.4914, dtype=torch.float64)\n",
      "7.798096418380737\n",
      "current at batch: 145 tensor(5.4881, dtype=torch.float64)\n",
      "7.820652723312378\n",
      "current at batch: 146 tensor(5.7666, dtype=torch.float64)\n",
      "8.19538164138794\n",
      "current at batch: 147 tensor(5.3402, dtype=torch.float64)\n",
      "7.936208724975586\n",
      "current at batch: 148 tensor(5.3895, dtype=torch.float64)\n",
      "7.75721549987793\n",
      "current at batch: 149 tensor(5.4455, dtype=torch.float64)\n",
      "8.119501829147339\n",
      "current at batch: 150 tensor(5.5631, dtype=torch.float64)\n",
      "8.471185445785522\n",
      "current at batch: 151 tensor(5.4635, dtype=torch.float64)\n",
      "7.9007463455200195\n",
      "current at batch: 152 tensor(5.4151, dtype=torch.float64)\n",
      "8.112891912460327\n",
      "current at batch: 153 tensor(5.5319, dtype=torch.float64)\n",
      "7.98577094078064\n",
      "current at batch: 154 tensor(5.4814, dtype=torch.float64)\n",
      "8.001319885253906\n",
      "current at batch: 155 tensor(5.5198, dtype=torch.float64)\n",
      "7.892664670944214\n",
      "current at batch: 156 tensor(5.4960, dtype=torch.float64)\n",
      "8.323033809661865\n",
      "current at batch: 157 tensor(5.2098, dtype=torch.float64)\n",
      "8.128507614135742\n",
      "current at batch: 158 tensor(5.7137, dtype=torch.float64)\n",
      "7.39180588722229\n",
      "current at batch: 159 tensor(5.0947, dtype=torch.float64)\n",
      "8.027323007583618\n",
      "current at batch: 160 tensor(5.7180, dtype=torch.float64)\n",
      "8.112359523773193\n",
      "current at batch: 161 tensor(5.3089, dtype=torch.float64)\n",
      "7.951852083206177\n",
      "current at batch: 162 tensor(5.5826, dtype=torch.float64)\n",
      "8.001384258270264\n",
      "current at batch: 163 tensor(5.2911, dtype=torch.float64)\n",
      "8.338643312454224\n",
      "current at batch: 164 tensor(5.3260, dtype=torch.float64)\n",
      "7.960137605667114\n",
      "current at batch: 165 tensor(5.5031, dtype=torch.float64)\n",
      "8.10588788986206\n",
      "current at batch: 166 tensor(5.4705, dtype=torch.float64)\n",
      "7.622178077697754\n",
      "current at batch: 167 tensor(5.4706, dtype=torch.float64)\n",
      "7.786738157272339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 168 tensor(5.1976, dtype=torch.float64)\n",
      "7.511229515075684\n",
      "current at batch: 169 tensor(5.2838, dtype=torch.float64)\n",
      "8.286005973815918\n",
      "current at batch: 170 tensor(5.5338, dtype=torch.float64)\n",
      "8.29363226890564\n",
      "current at batch: 171 tensor(5.5555, dtype=torch.float64)\n",
      "8.307960510253906\n",
      "current at batch: 172 tensor(5.3703, dtype=torch.float64)\n",
      "8.284945487976074\n",
      "current at batch: 173 tensor(5.7242, dtype=torch.float64)\n",
      "8.196887016296387\n",
      "current at batch: 174 tensor(5.4407, dtype=torch.float64)\n",
      "7.833191156387329\n",
      "current at batch: 175 tensor(5.4807, dtype=torch.float64)\n",
      "7.884318590164185\n",
      "current at batch: 176 tensor(5.4341, dtype=torch.float64)\n",
      "7.836674451828003\n",
      "current at batch: 177 tensor(5.4432, dtype=torch.float64)\n",
      "7.957776308059692\n",
      "current at batch: 178 tensor(5.5924, dtype=torch.float64)\n",
      "8.086153745651245\n",
      "current at batch: 179 tensor(5.4934, dtype=torch.float64)\n",
      "7.769753456115723\n",
      "current at batch: 180 tensor(5.6509, dtype=torch.float64)\n",
      "7.9362592697143555\n",
      "current at batch: 181 tensor(5.6003, dtype=torch.float64)\n",
      "8.214552164077759\n",
      "current at batch: 182 tensor(5.5823, dtype=torch.float64)\n",
      "8.350045680999756\n",
      "current at batch: 183 tensor(5.3882, dtype=torch.float64)\n",
      "8.217182397842407\n",
      "current at batch: 184 tensor(5.7356, dtype=torch.float64)\n",
      "8.046828031539917\n",
      "current at batch: 185 tensor(5.1030, dtype=torch.float64)\n",
      "7.703671455383301\n",
      "current at batch: 186 tensor(5.7971, dtype=torch.float64)\n",
      "8.098363637924194\n",
      "current at batch: 187 tensor(5.3398, dtype=torch.float64)\n",
      "8.289713144302368\n",
      "current at batch: 188 tensor(5.3400, dtype=torch.float64)\n",
      "7.904273986816406\n",
      "current at batch: 189 tensor(4.7494, dtype=torch.float64)\n",
      "3.802852153778076\n",
      "epoch =  6 \n",
      " tensor(1028.7298, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Training process!\n",
    "import time\n",
    "# setting training parameters\n",
    "batchsize = 100\n",
    "epoch = 7\n",
    "lr = 5000\n",
    "lr_nmf = 5000\n",
    "lr_cl = 5000\n",
    "loss_lst = []\n",
    "# train!\n",
    "for epo in range(epoch):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchsize, shuffle = True)\n",
    "    total_loss = 0\n",
    "    for (i, (inputs, label)) in enumerate(dataloader):\n",
    "        t1 = time.time()\n",
    "        inputs = inputs.view([inputs.shape[0], inputs.shape[2]])\n",
    "        label = label.view([label.shape[0], -1])\n",
    "        inputs, label = Variable(inputs), Variable(label)\n",
    "       #train the lsqnonneg layers\n",
    "        net.zero_grad()\n",
    "        S_lst = net(inputs)\n",
    "        S = S_lst[-1]\n",
    "        B = pinv(S)\n",
    "        B = torch.mm(pinv(S),label)\n",
    "        pred = torch.mm(S,B)\n",
    "        loss = loss_func(inputs, S_lst,list(net.lsqnonneglst.parameters()),pred,label)\n",
    "        loss.backward()\n",
    "        loss_lst.append(loss.data)\n",
    "        total_loss += loss.data\n",
    "        print('current at batch:', i+1, loss.data)\n",
    "        t2 = time.time()\n",
    "        print(t2 - t1)\n",
    "        for A in net.parameters():\n",
    "            A.data = A.data.sub_(lr*A.grad.data)\n",
    "        for A in net.lsqnonneglst.parameters():\n",
    "            A.data = A.data.clamp(min = 0)\n",
    "    print('epoch = ', epo, '\\n', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "np.savez(save_PATH + save_filename,\n",
    "         param_lst = list(net.parameters()), loss_lst = loss_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num =  189 \n",
      "\n",
      "current at batch: 0\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Dropbox\\Deep NMF\\Code\\Cathy Code\\Full-Backprop-Test-on-20-News-1\\auxillary_functions.py\u001b[0m in \u001b[0;36mget_whole_output\u001b[1;34m(net, dataset, param_lst)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mtotal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Deep NMF\\Code\\Cathy Code\\Full-Backprop-Test-on-20-News-1\\twenty_news_group_data_loading.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.uint16. The only supported types are: double, float, float16, int64, int32, and uint8.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-c7eeddd548b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# # Get the whole output of the whole dataset (running forward propagation on the whole dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_whole_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m accuracy = torch.sum(torch.argmax(pred, 1) \n",
      "\u001b[1;32m~\\Dropbox\\Deep NMF\\Code\\Cathy Code\\Full-Backprop-Test-on-20-News-1\\auxillary_functions.py\u001b[0m in \u001b[0;36mget_whole_output\u001b[1;34m(net, dataset, param_lst)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mtotal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mtotal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Deep NMF\\Code\\Cathy Code\\Full-Backprop-Test-on-20-News-1\\twenty_news_group_data_loading.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    844\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \"\"\"\n\u001b[1;32m--> 846\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# plot the loss curve\n",
    "plt.plot(loss_lst)\n",
    "plt.show()\n",
    "# # Get the whole output of the whole dataset (running forward propagation on the whole dataset)\n",
    "S, pred = get_whole_output(net, dataset)\n",
    "# Get the accuracy\n",
    "accuracy = torch.sum(torch.argmax(pred, 1) \n",
    "                      == torch.argmax(torch.from_numpy(Y),1)).float()/len(dataset)\n",
    "# print(accuracy)\n",
    "# # Get the reconstruction error\n",
    "# A_np = net.lsqnonneglst[0].A.data.numpy()\n",
    "# S_np = S.data.numpy()\n",
    "# fro_error, fro_X = calc_reconstruction_error(data_input, A_np, S_np)\n",
    "# print(fro_error/fro_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "np.savez(save_PATH + save_filename, S = S, pred = pred,\n",
    "         param_lst = list(net.parameters()), loss_lst = loss_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current at batch: 100\n",
      "current at batch: 200\n",
      "current at batch: 300\n",
      "current at batch: 400\n",
      "current at batch: 500\n",
      "current at batch: 600\n",
      "current at batch: 700\n",
      "current at batch: 800\n",
      "current at batch: 900\n",
      "current at batch: 1000\n",
      "current at batch: 1100\n",
      "current at batch: 1200\n",
      "current at batch: 1300\n",
      "current at batch: 1400\n",
      "current at batch: 1500\n",
      "current at batch: 1600\n",
      "current at batch: 1700\n",
      "current at batch: 1800\n",
      "current at batch: 1900\n",
      "current at batch: 2000\n",
      "current at batch: 2100\n",
      "current at batch: 2200\n",
      "current at batch: 2300\n",
      "current at batch: 2400\n",
      "current at batch: 2500\n",
      "current at batch: 2600\n",
      "current at batch: 2700\n",
      "current at batch: 2800\n",
      "current at batch: 2900\n",
      "current at batch: 3000\n",
      "current at batch: 3100\n",
      "current at batch: 3200\n",
      "current at batch: 3300\n",
      "current at batch: 3400\n",
      "current at batch: 3500\n",
      "current at batch: 3600\n",
      "current at batch: 3700\n",
      "current at batch: 3800\n",
      "current at batch: 3900\n",
      "current at batch: 4000\n",
      "current at batch: 4100\n",
      "current at batch: 4200\n",
      "current at batch: 4300\n",
      "current at batch: 4400\n",
      "current at batch: 4500\n",
      "current at batch: 4600\n",
      "current at batch: 4700\n",
      "current at batch: 4800\n",
      "current at batch: 4900\n",
      "current at batch: 5000\n",
      "current at batch: 5100\n",
      "current at batch: 5200\n",
      "current at batch: 5300\n",
      "current at batch: 5400\n",
      "current at batch: 5500\n",
      "current at batch: 5600\n",
      "current at batch: 5700\n",
      "current at batch: 5800\n",
      "current at batch: 5900\n",
      "current at batch: 6000\n",
      "current at batch: 6100\n",
      "current at batch: 6200\n",
      "current at batch: 6300\n",
      "current at batch: 6400\n",
      "current at batch: 6500\n",
      "current at batch: 6600\n",
      "current at batch: 6700\n",
      "current at batch: 6800\n",
      "current at batch: 6900\n",
      "current at batch: 7000\n",
      "current at batch: 7100\n",
      "current at batch: 7200\n",
      "current at batch: 7300\n",
      "current at batch: 7400\n",
      "current at batch: 7500\n",
      "current at batch: 7600\n",
      "current at batch: 7700\n",
      "current at batch: 7800\n",
      "current at batch: 7900\n",
      "current at batch: 8000\n",
      "current at batch: 8100\n",
      "current at batch: 8200\n",
      "current at batch: 8300\n",
      "current at batch: 8400\n",
      "current at batch: 8500\n",
      "current at batch: 8600\n",
      "current at batch: 8700\n",
      "current at batch: 8800\n",
      "current at batch: 8900\n",
      "current at batch: 9000\n",
      "current at batch: 9100\n",
      "current at batch: 9200\n",
      "current at batch: 9300\n",
      "current at batch: 9400\n",
      "current at batch: 9500\n",
      "current at batch: 9600\n",
      "current at batch: 9700\n",
      "current at batch: 9800\n",
      "current at batch: 9900\n",
      "current at batch: 10000\n",
      "current at batch: 10100\n",
      "current at batch: 10200\n",
      "current at batch: 10300\n",
      "current at batch: 10400\n",
      "current at batch: 10500\n",
      "current at batch: 10600\n",
      "current at batch: 10700\n",
      "current at batch: 10800\n",
      "current at batch: 10900\n",
      "current at batch: 11000\n",
      "current at batch: 11100\n",
      "current at batch: 11200\n",
      "current at batch: 11300\n",
      "current at batch: 11400\n",
      "current at batch: 11500\n",
      "current at batch: 11600\n",
      "current at batch: 11700\n",
      "current at batch: 11800\n",
      "current at batch: 11900\n",
      "current at batch: 12000\n",
      "current at batch: 12100\n",
      "current at batch: 12200\n",
      "current at batch: 12300\n",
      "current at batch: 12400\n",
      "current at batch: 12500\n",
      "current at batch: 12600\n",
      "current at batch: 12700\n",
      "current at batch: 12800\n",
      "current at batch: 12900\n",
      "current at batch: 13000\n",
      "current at batch: 13100\n",
      "current at batch: 13200\n",
      "current at batch: 13300\n",
      "current at batch: 13400\n",
      "current at batch: 13500\n",
      "current at batch: 13600\n",
      "current at batch: 13700\n",
      "current at batch: 13800\n",
      "current at batch: 13900\n",
      "current at batch: 14000\n",
      "current at batch: 14100\n",
      "current at batch: 14200\n",
      "current at batch: 14300\n",
      "current at batch: 14400\n",
      "current at batch: 14500\n",
      "current at batch: 14600\n",
      "current at batch: 14700\n",
      "current at batch: 14800\n",
      "current at batch: 14900\n",
      "current at batch: 15000\n",
      "current at batch: 15100\n",
      "current at batch: 15200\n",
      "current at batch: 15300\n",
      "current at batch: 15400\n",
      "current at batch: 15500\n",
      "current at batch: 15600\n",
      "current at batch: 15700\n",
      "current at batch: 15800\n",
      "current at batch: 15900\n",
      "current at batch: 16000\n",
      "current at batch: 16100\n",
      "current at batch: 16200\n",
      "current at batch: 16300\n",
      "current at batch: 16400\n",
      "current at batch: 16500\n",
      "current at batch: 16600\n",
      "current at batch: 16700\n",
      "current at batch: 16800\n",
      "current at batch: 16900\n",
      "current at batch: 17000\n",
      "current at batch: 17100\n",
      "current at batch: 17200\n",
      "current at batch: 17300\n",
      "current at batch: 17400\n",
      "current at batch: 17500\n",
      "current at batch: 17600\n",
      "current at batch: 17700\n",
      "current at batch: 17800\n",
      "current at batch: 17900\n",
      "current at batch: 18000\n",
      "current at batch: 18100\n",
      "current at batch: 18200\n",
      "current at batch: 18300\n",
      "current at batch: 18400\n",
      "current at batch: 18500\n",
      "current at batch: 18600\n",
      "current at batch: 18700\n",
      "current at batch: 18800\n"
     ]
    }
   ],
   "source": [
    "history = Writer()\n",
    "for A in net.parameters():\n",
    "    A.requires_grad = False\n",
    "n = 18800\n",
    "for i in range(n):\n",
    "    if (i+1)%100 == 0:\n",
    "        print('current at batch:', i+1)\n",
    "    total_data = dataset[i]\n",
    "    inputs,label = total_data\n",
    "    S_lst = net(inputs)\n",
    "    S = S_lst[-1]\n",
    "    B = pinv(S)\n",
    "    B = torch.mm(pinv(S),label)\n",
    "    pred = torch.mm(S,B)\n",
    "    history.add_tensor('S', S)\n",
    "    history.add_tensor('pred',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = history.get('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18800, 20])\n"
     ]
    }
   ],
   "source": [
    "S1 = torch.cat(S,0)\n",
    "print(S1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_np = S1.numpy()\n",
    "inv_S = np.linalg.pinv(S_np)\n",
    "Y_sub = Y[0:n,:]\n",
    "Y_pred = S_np@(inv_S@Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504255319148937"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(Y_pred,1) == np.argmax(Y_sub,1))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18800, 20)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[   0.9001,    1.9379,    1.1450,  ...,    1.1023,    0.9945,\n",
       "            0.5959],\n",
       "        [   3.0226,    9.2815,    0.8735,  ...,    2.3046,    1.4155,\n",
       "            0.4899],\n",
       "        [   0.0000,    4.1337,    0.6312,  ...,    3.3140,    1.4027,\n",
       "            1.1674],\n",
       "        ...,\n",
       "        [   3.4451,    0.2869,    1.1735,  ...,    1.4506,    1.6482,\n",
       "            0.7013],\n",
       "        [   1.5524,    0.0000,    1.0176,  ...,    1.2545,    1.1633,\n",
       "            1.4122],\n",
       "        [   3.1566,   30.1311,    0.9568,  ...,    1.2182,    0.7534,\n",
       "            0.2872]], dtype=torch.float64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.lsqnonneglst[0].A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
